{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f398a338",
   "metadata": {},
   "source": [
    "# Proceso ETL: Una Guía Práctica con Julia\n",
    "\n",
    "## 📚 Objetivos de Aprendizaje\n",
    "\n",
    "Al completar este notebook, los estudiantes serán capaces de:\n",
    "- Comprender los conceptos fundamentales de ETL (Extract, Transform, Load)\n",
    "- Implementar cada fase del proceso ETL usando Julia y DataFrames.jl\n",
    "- Aplicar técnicas de limpieza y transformación de datos\n",
    "- Manejar errores comunes en pipelines de datos\n",
    "- Diseñar pipelines ETL escalables y mantenibles\n",
    "\n",
    "## 🎯 ¿Qué es ETL?\n",
    "\n",
    "**ETL** es un proceso fundamental en ingeniería de datos que permite:\n",
    "- **Extract (Extraer)**: Obtener datos de múltiples fuentes\n",
    "- **Transform (Transformar)**: Limpiar, validar y estructurar los datos\n",
    "- **Load (Cargar)**: Almacenar los datos procesados en el destino final\n",
    "\n",
    "Este notebook es una guía completa que cubre desde conceptos básicos hasta temas avanzados, con ejemplos prácticos y ejercicios para trabajar con pipelines ETL en entornos reales de ingeniería de datos usando **Julia**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecfbacc",
   "metadata": {},
   "source": [
    "## El Pipeline ETL de Referencia\n",
    "\n",
    "Nos basaremos en la siguiente estructura de pipeline, que representa un flujo ETL clásico:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c388acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "function etl_pipeline()\n",
    "    # 1. Extraer datos desde una fuente original\n",
    "    raw_data = extract_from_database()\n",
    "    \n",
    "    # 2. Transformar los datos aplicando reglas y limpiando\n",
    "    cleaned_data = apply_business_rules(raw_data)\n",
    "    \n",
    "    # 3. Normalizar el esquema para que sea consistente\n",
    "    structured_data = normalize_schema(cleaned_data)\n",
    "    \n",
    "    # 4. Cargar los datos transformados a su destino final\n",
    "    load_to_warehouse(structured_data)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e658455",
   "metadata": {},
   "source": [
    "A continuación, implementaremos cada una de estas funciones con ejemplos claros."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83b5c91",
   "metadata": {},
   "source": [
    "## 📦 Librerías y Configuración Inicial\n",
    "\n",
    "Para nuestros ejemplos, usaremos las siguientes librerías esenciales en ingeniería de datos con Julia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11e912fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Librerías cargadas exitosamente\n"
     ]
    }
   ],
   "source": [
    "# Instalar paquetes necesarios (ejecutar solo la primera vez)\n",
    "using Pkg\n",
    "# Pkg.add([\"DataFrames\", \"CSV\", \"Dates\", \"JSON3\", \"Logging\", \"Statistics\"])\n",
    "\n",
    "# Cargar librerías\n",
    "using DataFrames        # Manipulación y análisis de datos\n",
    "using CSV              # Lectura y escritura de archivos CSV\n",
    "using Dates            # Manejo de fechas y tiempos\n",
    "using JSON3            # Manejo de datos JSON\n",
    "using Logging          # Sistema de logs\n",
    "using Statistics       # Funciones estadísticas básicas\n",
    "\n",
    "# Configuración de logging para monitorear el pipeline\n",
    "logger = ConsoleLogger(stdout, Logging.Info)\n",
    "global_logger(logger)\n",
    "\n",
    "println(\"✅ Librerías cargadas exitosamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2683b1",
   "metadata": {},
   "source": [
    "## 🧠 Conceptos Fundamentales de ETL\n",
    "\n",
    "### Tipos de Procesamiento ETL\n",
    "\n",
    "1. **Batch Processing (Procesamiento por lotes)**\n",
    "   - Procesa grandes volúmenes de datos en intervalos programados\n",
    "   - Ideal para reportes diarios, semanales o mensuales\n",
    "   - Ejemplo: Procesar todas las transacciones del día anterior\n",
    "\n",
    "2. **Real-time Processing (Procesamiento en tiempo real)**\n",
    "   - Procesa datos tan pronto como llegan\n",
    "   - Crítico para aplicaciones que requieren respuesta inmediata\n",
    "   - Ejemplo: Detección de fraude en transacciones\n",
    "\n",
    "3. **Near Real-time Processing (Procesamiento casi en tiempo real)**\n",
    "   - Procesa datos con un pequeño retraso (segundos o minutos)\n",
    "   - Balance entre velocidad y eficiencia de recursos\n",
    "   - Ejemplo: Actualización de dashboards cada 5 minutos\n",
    "\n",
    "### Calidad de Datos - Las 6 Dimensiones\n",
    "\n",
    "1. **Exactitud**: Los datos reflejan la realidad\n",
    "2. **Completitud**: No hay valores faltantes críticos\n",
    "3. **Consistencia**: Los datos son coherentes entre sistemas\n",
    "4. **Validez**: Los datos cumplen con reglas de negocio\n",
    "5. **Unicidad**: No hay duplicados no deseados\n",
    "6. **Actualidad**: Los datos están actualizados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5101e2",
   "metadata": {},
   "source": [
    "## 1. Extract: Extracción de Datos\n",
    "\n",
    "### 🎯 Objetivos de la Fase Extract\n",
    "- Conectar con múltiples fuentes de datos\n",
    "- Extraer datos de manera eficiente\n",
    "- Manejar diferentes formatos y protocolos\n",
    "- Implementar estrategias de extracción incremental\n",
    "\n",
    "### 📊 Fuentes de Datos Comunes\n",
    "- **Bases de datos relacionales**: MySQL, PostgreSQL, SQL Server\n",
    "- **Bases de datos NoSQL**: MongoDB, Cassandra, Redis\n",
    "- **Archivos**: CSV, JSON, XML, Parquet\n",
    "- **APIs REST**: Servicios web y microservicios\n",
    "- **Sistemas de streaming**: Kafka, Kinesis\n",
    "\n",
    "**Ejemplo:** Vamos a simular la extracción de datos de una base de datos SQL creando un DataFrame con datos de ejemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "986ce113",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "extract_from_database (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function extract_from_database()\n",
    "    \"\"\"\n",
    "    Simula la extracción de datos desde una base de datos.\n",
    "    En un entorno real, esto se conectaría a una base de datos real.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: Datos crudos extraídos\n",
    "    \"\"\"\n",
    "    try\n",
    "        @info \"Iniciando extracción de datos...\"\n",
    "        \n",
    "        # Simulamos datos con problemas típicos de calidad\n",
    "        data = DataFrame(\n",
    "            ID_USER = [101, 102, 103, 104, 105, 106],\n",
    "            user_name = [\"Ana\", \"Luis\", \"Marta\", \"Juan\", \"Eva\", missing],  # Valor nulo\n",
    "            registration_date = [\"2025-01-15\", \"2025-02-20\", \"2025-03-01\", \"2025-04-10\", \"2025-05-19\", \"2025-06-30\"],\n",
    "            total_spent = [150.5, 80.0, -999.0, 200.0, 45.25, \"invalid\"],  # Valor inválido\n",
    "            country_code = [\"ES\", \"MX\", \"ES\", \"CO\", \"es\", \"ES\"],  # Inconsistencia en mayúsculas\n",
    "            email = [\"ana@email.com\", \"luis@email.com\", \"marta@email.com\", \"juan@email.com\", \"eva@email.com\", \"pedro@email.com\"]\n",
    "        )\n",
    "        \n",
    "        println(\"--- 1. Datos Crudos Extraídos ---\")\n",
    "        println(\"Registros extraídos: \", nrow(data))\n",
    "        println(\"Columnas: \", names(data))\n",
    "        println(\"\\nPrimeras filas:\")\n",
    "        println(data)\n",
    "        println(\"\\nInformación del DataFrame:\")\n",
    "        println(\"Tipos de datos: \", eltype.(eachcol(data)))\n",
    "        println(\"\")\n",
    "        \n",
    "        @info \"Extracción completada: $(nrow(data)) registros\"\n",
    "        return data\n",
    "        \n",
    "    catch e\n",
    "        @error \"Error en la extracción: $e\"\n",
    "        rethrow(e)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af4c3f1",
   "metadata": {},
   "source": [
    "### ▶️ Ejecutemos la Extracción\n",
    "\n",
    "Ahora vamos a ejecutar la función de extracción para ver los datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df7c0a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mIniciando extracción de datos...\n",
      "--- 1. Datos Crudos Extraídos ---\n",
      "Registros extraídos: 6\n",
      "Columnas: [\"ID_USER\", \"user_name\", \"registration_date\", \"total_spent\", \"country_code\", \"email\"]\n",
      "\n",
      "Primeras filas:\n",
      "\u001b[1m6×6 DataFrame\u001b[0m\n",
      "\u001b[1m Row \u001b[0m│\u001b[1m ID_USER \u001b[0m\u001b[1m user_name \u001b[0m\u001b[1m registration_date \u001b[0m\u001b[1m total_spent \u001b[0m\u001b[1m country_code \u001b[0m\u001b[1m email           \u001b[0m\n",
      "     │\u001b[90m Int64   \u001b[0m\u001b[90m String?   \u001b[0m\u001b[90m String            \u001b[0m\u001b[90m Any         \u001b[0m\u001b[90m String       \u001b[0m\u001b[90m String          \u001b[0m\n",
      "─────┼───────────────────────────────────────────────────────────────────────────────────\n",
      "   1 │     101  Ana        2025-01-15         150.5        ES            ana@email.com\n",
      "   2 │     102  Luis       2025-02-20         80.0         MX            luis@email.com\n",
      "   3 │     103  Marta      2025-03-01         -999.0       ES            marta@email.com\n",
      "   4 │     104  Juan       2025-04-10         200.0        CO            juan@email.com\n",
      "   5 │     105  Eva        2025-05-19         45.25        es            eva@email.com\n",
      "   6 │     106 \u001b[90m missing   \u001b[0m 2025-06-30         invalid      ES            pedro@email.com\n",
      "\n",
      "Información del DataFrame:\n",
      "Tipos de datos: Type[Int64, Union{Missing, String}, String, Any, String, String]\n",
      "\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mExtracción completada: 6 registros\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><div style = \"float: left;\"><span>6×6 DataFrame</span></div><div style = \"clear: both;\"></div></div><div class = \"data-frame\" style = \"overflow-x: scroll;\"><table class = \"data-frame\" style = \"margin-bottom: 6px;\"><thead><tr class = \"header\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">Row</th><th style = \"text-align: left;\">ID_USER</th><th style = \"text-align: left;\">user_name</th><th style = \"text-align: left;\">registration_date</th><th style = \"text-align: left;\">total_spent</th><th style = \"text-align: left;\">country_code</th><th style = \"text-align: left;\">email</th></tr><tr class = \"subheader headerLastRow\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\"></th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Union{Missing, String}\" style = \"text-align: left;\">String?</th><th title = \"String\" style = \"text-align: left;\">String</th><th title = \"Any\" style = \"text-align: left;\">Any</th><th title = \"String\" style = \"text-align: left;\">String</th><th title = \"String\" style = \"text-align: left;\">String</th></tr></thead><tbody><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1</td><td style = \"text-align: right;\">101</td><td style = \"text-align: left;\">Ana</td><td style = \"text-align: left;\">2025-01-15</td><td style = \"text-align: left;\">150.5</td><td style = \"text-align: left;\">ES</td><td style = \"text-align: left;\">ana@email.com</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">2</td><td style = \"text-align: right;\">102</td><td style = \"text-align: left;\">Luis</td><td style = \"text-align: left;\">2025-02-20</td><td style = \"text-align: left;\">80.0</td><td style = \"text-align: left;\">MX</td><td style = \"text-align: left;\">luis@email.com</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">3</td><td style = \"text-align: right;\">103</td><td style = \"text-align: left;\">Marta</td><td style = \"text-align: left;\">2025-03-01</td><td style = \"text-align: left;\">-999.0</td><td style = \"text-align: left;\">ES</td><td style = \"text-align: left;\">marta@email.com</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">4</td><td style = \"text-align: right;\">104</td><td style = \"text-align: left;\">Juan</td><td style = \"text-align: left;\">2025-04-10</td><td style = \"text-align: left;\">200.0</td><td style = \"text-align: left;\">CO</td><td style = \"text-align: left;\">juan@email.com</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">5</td><td style = \"text-align: right;\">105</td><td style = \"text-align: left;\">Eva</td><td style = \"text-align: left;\">2025-05-19</td><td style = \"text-align: left;\">45.25</td><td style = \"text-align: left;\">es</td><td style = \"text-align: left;\">eva@email.com</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">6</td><td style = \"text-align: right;\">106</td><td style = \"font-style: italic; text-align: left;\">missing</td><td style = \"text-align: left;\">2025-06-30</td><td style = \"text-align: left;\">invalid</td><td style = \"text-align: left;\">ES</td><td style = \"text-align: left;\">pedro@email.com</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccccc}\n",
       "\t& ID\\_USER & user\\_name & registration\\_date & total\\_spent & country\\_code & email\\\\\n",
       "\t\\hline\n",
       "\t& Int64 & String? & String & Any & String & String\\\\\n",
       "\t\\hline\n",
       "\t1 & 101 & Ana & 2025-01-15 & 150.5 & ES & ana@email.com \\\\\n",
       "\t2 & 102 & Luis & 2025-02-20 & 80.0 & MX & luis@email.com \\\\\n",
       "\t3 & 103 & Marta & 2025-03-01 & -999.0 & ES & marta@email.com \\\\\n",
       "\t4 & 104 & Juan & 2025-04-10 & 200.0 & CO & juan@email.com \\\\\n",
       "\t5 & 105 & Eva & 2025-05-19 & 45.25 & es & eva@email.com \\\\\n",
       "\t6 & 106 & \\emph{missing} & 2025-06-30 & invalid & ES & pedro@email.com \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m6×6 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m ID_USER \u001b[0m\u001b[1m user_name \u001b[0m\u001b[1m registration_date \u001b[0m\u001b[1m total_spent \u001b[0m\u001b[1m country_code \u001b[0m\u001b[1m email\u001b[0m ⋯\n",
       "     │\u001b[90m Int64   \u001b[0m\u001b[90m String?   \u001b[0m\u001b[90m String            \u001b[0m\u001b[90m Any         \u001b[0m\u001b[90m String       \u001b[0m\u001b[90m Strin\u001b[0m ⋯\n",
       "─────┼──────────────────────────────────────────────────────────────────────────\n",
       "   1 │     101  Ana        2025-01-15         150.5        ES            ana@e ⋯\n",
       "   2 │     102  Luis       2025-02-20         80.0         MX            luis@\n",
       "   3 │     103  Marta      2025-03-01         -999.0       ES            marta\n",
       "   4 │     104  Juan       2025-04-10         200.0        CO            juan@\n",
       "   5 │     105  Eva        2025-05-19         45.25        es            eva@e ⋯\n",
       "   6 │     106 \u001b[90m missing   \u001b[0m 2025-06-30         invalid      ES            pedro\n",
       "\u001b[36m                                                                1 column omitted\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Ejecutar la función de extracción\n",
    "raw_data = extract_from_database()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3b0ba8",
   "metadata": {},
   "source": [
    "### 📁 Ejemplo: Extracción desde Archivo CSV\n",
    "\n",
    "Veamos cómo extraer datos desde un archivo CSV (otra fuente común):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94c6f9f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Extracción desde CSV ---\n",
      "\u001b[1m5×4 DataFrame\u001b[0m\n",
      "\u001b[1m Row \u001b[0m│\u001b[1m product_id \u001b[0m\u001b[1m product_name \u001b[0m\u001b[1m price   \u001b[0m\u001b[1m category    \u001b[0m\n",
      "     │\u001b[90m Int64      \u001b[0m\u001b[90m String15     \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m String15    \u001b[0m\n",
      "─────┼────────────────────────────────────────────────\n",
      "   1 │          1  Laptop         999.99  Electronics\n",
      "   2 │          2  Mouse           25.5   Accessories\n",
      "   3 │          3  Keyboard        75.0   Accessories\n",
      "   4 │          4  Monitor        299.99  Electronics\n",
      "   5 │          5  Headphones      89.99  Accessories\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><div style = \"float: left;\"><span>5×4 DataFrame</span></div><div style = \"clear: both;\"></div></div><div class = \"data-frame\" style = \"overflow-x: scroll;\"><table class = \"data-frame\" style = \"margin-bottom: 6px;\"><thead><tr class = \"header\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">Row</th><th style = \"text-align: left;\">product_id</th><th style = \"text-align: left;\">product_name</th><th style = \"text-align: left;\">price</th><th style = \"text-align: left;\">category</th></tr><tr class = \"subheader headerLastRow\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\"></th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"String15\" style = \"text-align: left;\">String15</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"String15\" style = \"text-align: left;\">String15</th></tr></thead><tbody><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: left;\">Laptop</td><td style = \"text-align: right;\">999.99</td><td style = \"text-align: left;\">Electronics</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">2</td><td style = \"text-align: right;\">2</td><td style = \"text-align: left;\">Mouse</td><td style = \"text-align: right;\">25.5</td><td style = \"text-align: left;\">Accessories</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">3</td><td style = \"text-align: right;\">3</td><td style = \"text-align: left;\">Keyboard</td><td style = \"text-align: right;\">75.0</td><td style = \"text-align: left;\">Accessories</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">4</td><td style = \"text-align: right;\">4</td><td style = \"text-align: left;\">Monitor</td><td style = \"text-align: right;\">299.99</td><td style = \"text-align: left;\">Electronics</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">5</td><td style = \"text-align: right;\">5</td><td style = \"text-align: left;\">Headphones</td><td style = \"text-align: right;\">89.99</td><td style = \"text-align: left;\">Accessories</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccc}\n",
       "\t& product\\_id & product\\_name & price & category\\\\\n",
       "\t\\hline\n",
       "\t& Int64 & String15 & Float64 & String15\\\\\n",
       "\t\\hline\n",
       "\t1 & 1 & Laptop & 999.99 & Electronics \\\\\n",
       "\t2 & 2 & Mouse & 25.5 & Accessories \\\\\n",
       "\t3 & 3 & Keyboard & 75.0 & Accessories \\\\\n",
       "\t4 & 4 & Monitor & 299.99 & Electronics \\\\\n",
       "\t5 & 5 & Headphones & 89.99 & Accessories \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m5×4 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m product_id \u001b[0m\u001b[1m product_name \u001b[0m\u001b[1m price   \u001b[0m\u001b[1m category    \u001b[0m\n",
       "     │\u001b[90m Int64      \u001b[0m\u001b[90m String15     \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m String15    \u001b[0m\n",
       "─────┼────────────────────────────────────────────────\n",
       "   1 │          1  Laptop         999.99  Electronics\n",
       "   2 │          2  Mouse           25.5   Accessories\n",
       "   3 │          3  Keyboard        75.0   Accessories\n",
       "   4 │          4  Monitor        299.99  Electronics\n",
       "   5 │          5  Headphones      89.99  Accessories"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function extract_from_csv(file_path=\"sample_products.csv\")\n",
    "    \"\"\"\n",
    "    Extrae datos desde un archivo CSV.\n",
    "    \n",
    "    Args:\n",
    "        file_path (String): Ruta del archivo CSV\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: Datos extraídos del CSV\n",
    "    \"\"\"\n",
    "    try\n",
    "        # Crear un archivo CSV de ejemplo\n",
    "        sample_data = DataFrame(\n",
    "            product_id = [1, 2, 3, 4, 5],\n",
    "            product_name = [\"Laptop\", \"Mouse\", \"Keyboard\", \"Monitor\", \"Headphones\"],\n",
    "            price = [999.99, 25.50, 75.00, 299.99, 89.99],\n",
    "            category = [\"Electronics\", \"Accessories\", \"Accessories\", \"Electronics\", \"Accessories\"]\n",
    "        )\n",
    "        \n",
    "        CSV.write(file_path, sample_data)\n",
    "        \n",
    "        # Leer el archivo CSV\n",
    "        df = CSV.read(file_path, DataFrame)\n",
    "        \n",
    "        println(\"--- Extracción desde CSV ---\")\n",
    "        println(df)\n",
    "        return df\n",
    "        \n",
    "    catch e\n",
    "        if isa(e, SystemError)\n",
    "            @error \"Archivo no encontrado: $file_path\"\n",
    "            return DataFrame()\n",
    "        else\n",
    "            @error \"Error leyendo CSV: $e\"\n",
    "            return DataFrame()\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "# Ejemplo de uso\n",
    "products_df = extract_from_csv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff34182",
   "metadata": {},
   "source": [
    "## 2. Transform: Aplicación de Reglas de Negocio\n",
    "\n",
    "### 🎯 Objetivos de la Fase Transform\n",
    "- Limpiar datos inconsistentes o erróneos\n",
    "- Validar que los datos cumplan reglas de negocio\n",
    "- Enriquecer datos con información adicional\n",
    "- Aplicar transformaciones matemáticas o lógicas\n",
    "- Normalizar formatos y estructuras\n",
    "\n",
    "### 🔧 Técnicas Comunes de Transformación\n",
    "- **Limpieza**: Eliminar duplicados, corregir errores tipográficos\n",
    "- **Validación**: Verificar rangos, formatos, tipos de datos\n",
    "- **Enriquecimiento**: Agregar datos calculados o de referencia\n",
    "- **Normalización**: Estandarizar formatos y escalas\n",
    "- **Agregación**: Resumir datos por grupos o períodos\n",
    "\n",
    "**Ejemplo:** Aplicaremos las siguientes reglas:\n",
    "1. Corregir valores erróneos: El valor `-999` en `total_spent` debe ser 0\n",
    "2. Corregir inconsistencias: El código de país `es` debe ser `ES`\n",
    "3. Filtrar datos: Solo usuarios con gasto mayor a 50\n",
    "4. Manejar valores nulos en nombres de usuario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85760317",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "apply_business_rules (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function apply_business_rules(raw_data::DataFrame)\n",
    "    \"\"\"\n",
    "    Aplica reglas de negocio y limpieza de datos.\n",
    "    \n",
    "    Args:\n",
    "        raw_data (DataFrame): Datos crudos a transformar\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: Datos limpios y transformados\n",
    "    \"\"\"\n",
    "    try\n",
    "        @info \"Iniciando transformación de datos...\"\n",
    "        cleaned_data = copy(raw_data)\n",
    "        \n",
    "        println(\"--- Análisis de Calidad de Datos ---\")\n",
    "        println(\"Registros iniciales: \", nrow(cleaned_data))\n",
    "        println(\"Valores nulos por columna:\")\n",
    "        for col in names(cleaned_data)\n",
    "            null_count = sum(ismissing.(cleaned_data[!, col]))\n",
    "            println(\"  $col: $null_count\")\n",
    "        end\n",
    "        println()\n",
    "        \n",
    "        # 1. Manejar valores nulos en user_name\n",
    "        null_names = sum(ismissing.(cleaned_data.user_name))\n",
    "        if null_names > 0\n",
    "            println(\"⚠️  Encontrados $null_names nombres de usuario nulos\")\n",
    "            cleaned_data.user_name = coalesce.(cleaned_data.user_name, \"Usuario_Desconocido\")\n",
    "        end\n",
    "        \n",
    "        # 2. Corregir valores erróneos en total_spent\n",
    "        # Convertir valores no numéricos a missing, luego reemplazar -999 por 0\n",
    "        cleaned_data.total_spent = [isa(val, Number) ? val : missing for val in cleaned_data.total_spent]\n",
    "        invalid_spent = sum(ismissing.(cleaned_data.total_spent))\n",
    "        if invalid_spent > 0\n",
    "            println(\"⚠️  Encontrados $invalid_spent valores inválidos en total_spent\")\n",
    "            cleaned_data.total_spent = coalesce.(cleaned_data.total_spent, 0.0)\n",
    "        end\n",
    "        \n",
    "        # Reemplazar -999 por 0\n",
    "        cleaned_data.total_spent = [val == -999.0 ? 0.0 : val for val in cleaned_data.total_spent]\n",
    "        \n",
    "        # 3. Normalizar códigos de país\n",
    "        cleaned_data.country_code = uppercase.(string.(cleaned_data.country_code))\n",
    "        \n",
    "        # 4. Aplicar regla de negocio: solo usuarios con gasto > 50\n",
    "        initial_count = nrow(cleaned_data)\n",
    "        cleaned_data = filter(row -> row.total_spent > 50, cleaned_data)\n",
    "        filtered_count = initial_count - nrow(cleaned_data)\n",
    "        \n",
    "        println(\"📊 Registros filtrados (gasto <= 50): $filtered_count\")\n",
    "        println(\"📊 Registros finales: \", nrow(cleaned_data))\n",
    "        \n",
    "        println(\"\\n--- 2. Datos Limpios (Reglas Aplicadas) ---\")\n",
    "        println(cleaned_data)\n",
    "        println(\"\")\n",
    "        \n",
    "        @info \"Transformación completada: $(nrow(cleaned_data)) registros válidos\"\n",
    "        return cleaned_data\n",
    "        \n",
    "    catch e\n",
    "        @error \"Error en la transformación: $e\"\n",
    "        rethrow(e)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc895c04",
   "metadata": {},
   "source": [
    "### ▶️ Ejecutemos la Transformación\n",
    "\n",
    "Ahora vamos a aplicar las reglas de negocio a nuestros datos extraídos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c9e44a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mIniciando transformación de datos...\n",
      "--- Análisis de Calidad de Datos ---\n",
      "Registros iniciales: 6\n",
      "Valores nulos por columna:\n",
      "  ID_USER: 0\n",
      "  user_name: 1\n",
      "  registration_date: 0\n",
      "  total_spent: 0\n",
      "  country_code: 0\n",
      "  email: 0\n",
      "\n",
      "⚠️  Encontrados 1 nombres de usuario nulos\n",
      "⚠️  Encontrados 1 valores inválidos en total_spent\n",
      "📊 Registros filtrados (gasto <= 50): 3\n",
      "📊 Registros finales: 3\n",
      "\n",
      "--- 2. Datos Limpios (Reglas Aplicadas) ---\n",
      "\u001b[1m3×6 DataFrame\u001b[0m\n",
      "\u001b[1m Row \u001b[0m│\u001b[1m ID_USER \u001b[0m\u001b[1m user_name \u001b[0m\u001b[1m registration_date \u001b[0m\u001b[1m total_spent \u001b[0m\u001b[1m country_code \u001b[0m\u001b[1m email          \u001b[0m\n",
      "     │\u001b[90m Int64   \u001b[0m\u001b[90m String    \u001b[0m\u001b[90m String            \u001b[0m\u001b[90m Float64     \u001b[0m\u001b[90m String       \u001b[0m\u001b[90m String         \u001b[0m\n",
      "─────┼──────────────────────────────────────────────────────────────────────────────────\n",
      "   1 │     101  Ana        2025-01-15               150.5  ES            ana@email.com\n",
      "   2 │     102  Luis       2025-02-20                80.0  MX            luis@email.com\n",
      "   3 │     104  Juan       2025-04-10               200.0  CO            juan@email.com\n",
      "\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mTransformación completada: 3 registros válidos\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><div style = \"float: left;\"><span>3×6 DataFrame</span></div><div style = \"clear: both;\"></div></div><div class = \"data-frame\" style = \"overflow-x: scroll;\"><table class = \"data-frame\" style = \"margin-bottom: 6px;\"><thead><tr class = \"header\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">Row</th><th style = \"text-align: left;\">ID_USER</th><th style = \"text-align: left;\">user_name</th><th style = \"text-align: left;\">registration_date</th><th style = \"text-align: left;\">total_spent</th><th style = \"text-align: left;\">country_code</th><th style = \"text-align: left;\">email</th></tr><tr class = \"subheader headerLastRow\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\"></th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"String\" style = \"text-align: left;\">String</th><th title = \"String\" style = \"text-align: left;\">String</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"String\" style = \"text-align: left;\">String</th><th title = \"String\" style = \"text-align: left;\">String</th></tr></thead><tbody><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1</td><td style = \"text-align: right;\">101</td><td style = \"text-align: left;\">Ana</td><td style = \"text-align: left;\">2025-01-15</td><td style = \"text-align: right;\">150.5</td><td style = \"text-align: left;\">ES</td><td style = \"text-align: left;\">ana@email.com</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">2</td><td style = \"text-align: right;\">102</td><td style = \"text-align: left;\">Luis</td><td style = \"text-align: left;\">2025-02-20</td><td style = \"text-align: right;\">80.0</td><td style = \"text-align: left;\">MX</td><td style = \"text-align: left;\">luis@email.com</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">3</td><td style = \"text-align: right;\">104</td><td style = \"text-align: left;\">Juan</td><td style = \"text-align: left;\">2025-04-10</td><td style = \"text-align: right;\">200.0</td><td style = \"text-align: left;\">CO</td><td style = \"text-align: left;\">juan@email.com</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccccc}\n",
       "\t& ID\\_USER & user\\_name & registration\\_date & total\\_spent & country\\_code & email\\\\\n",
       "\t\\hline\n",
       "\t& Int64 & String & String & Float64 & String & String\\\\\n",
       "\t\\hline\n",
       "\t1 & 101 & Ana & 2025-01-15 & 150.5 & ES & ana@email.com \\\\\n",
       "\t2 & 102 & Luis & 2025-02-20 & 80.0 & MX & luis@email.com \\\\\n",
       "\t3 & 104 & Juan & 2025-04-10 & 200.0 & CO & juan@email.com \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m3×6 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m ID_USER \u001b[0m\u001b[1m user_name \u001b[0m\u001b[1m registration_date \u001b[0m\u001b[1m total_spent \u001b[0m\u001b[1m country_code \u001b[0m\u001b[1m email\u001b[0m ⋯\n",
       "     │\u001b[90m Int64   \u001b[0m\u001b[90m String    \u001b[0m\u001b[90m String            \u001b[0m\u001b[90m Float64     \u001b[0m\u001b[90m String       \u001b[0m\u001b[90m Strin\u001b[0m ⋯\n",
       "─────┼──────────────────────────────────────────────────────────────────────────\n",
       "   1 │     101  Ana        2025-01-15               150.5  ES            ana@e ⋯\n",
       "   2 │     102  Luis       2025-02-20                80.0  MX            luis@\n",
       "   3 │     104  Juan       2025-04-10               200.0  CO            juan@\n",
       "\u001b[36m                                                                1 column omitted\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Ejecutar la transformación (necesitamos los datos de la extracción)\n",
    "if @isdefined(raw_data)\n",
    "    cleaned_data = apply_business_rules(raw_data)\n",
    "else\n",
    "    println(\"⚠️ Primero ejecuta la celda de extracción de datos\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50271d16",
   "metadata": {},
   "source": [
    "### 🔍 Técnicas Avanzadas de Transformación\n",
    "\n",
    "Veamos algunas técnicas adicionales de transformación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "be7445c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "advanced_transformations (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function advanced_transformations(df::DataFrame)\n",
    "    \"\"\"\n",
    "    Aplica transformaciones avanzadas a los datos.\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): DataFrame a transformar\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: DataFrame con transformaciones aplicadas\n",
    "    \"\"\"\n",
    "    df_transformed = copy(df)\n",
    "    \n",
    "    # 1. Crear columnas derivadas\n",
    "    df_transformed.registration_year = [parse(Int, split(date, \"-\")[1]) for date in df_transformed.registration_date]\n",
    "    df_transformed.registration_month = [parse(Int, split(date, \"-\")[2]) for date in df_transformed.registration_date]\n",
    "    \n",
    "    # 2. Categorizar gastos\n",
    "    function categorize_spending(amount)\n",
    "        if amount < 100\n",
    "            return \"Bajo\"\n",
    "        elseif amount < 200\n",
    "            return \"Medio\"\n",
    "        else\n",
    "            return \"Alto\"\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    df_transformed.spending_category = [categorize_spending(amount) for amount in df_transformed.total_spent]\n",
    "    \n",
    "    # 3. Validar emails (ejemplo básico)\n",
    "    if \"email\" in names(df_transformed)\n",
    "        df_transformed.email_valid = [occursin(\"@\", email) for email in df_transformed.email]\n",
    "    end\n",
    "    \n",
    "    println(\"--- Transformaciones Avanzadas ---\")\n",
    "    println(select(df_transformed, [:user_name, :total_spent, :spending_category, :registration_year]))\n",
    "    \n",
    "    return df_transformed\n",
    "end\n",
    "\n",
    "# Esta función se puede usar después de apply_business_rules\n",
    "# df_advanced = advanced_transformations(cleaned_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e089871",
   "metadata": {},
   "source": [
    "## 3. Transform: Normalización del Esquema\n",
    "\n",
    "### 🎯 Objetivos de la Normalización\n",
    "- Estandarizar nombres de columnas\n",
    "- Asegurar tipos de datos correctos\n",
    "- Mantener consistencia entre sistemas\n",
    "- Facilitar la integración con el destino\n",
    "\n",
    "El objetivo es asegurar que el esquema de los datos (nombres de columnas, tipos de datos) sea consistente.\n",
    "\n",
    "**Ejemplo:** Vamos a:\n",
    "1. Cambiar nombres de columnas a un formato estándar (snake_case)\n",
    "2. Asegurar que `registration_date` sea de tipo fecha\n",
    "3. Reordenar columnas según el esquema del destino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "90b2dd46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "normalize_schema (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function normalize_schema(cleaned_data::DataFrame)\n",
    "    \"\"\"\n",
    "    Normaliza el esquema de datos según estándares del data warehouse.\n",
    "    \n",
    "    Args:\n",
    "        cleaned_data (DataFrame): Datos limpios a normalizar\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: Datos con esquema normalizado\n",
    "    \"\"\"\n",
    "    try\n",
    "        @info \"Iniciando normalización del esquema...\"\n",
    "        structured_data = copy(cleaned_data)\n",
    "        \n",
    "        println(\"--- Análisis del Esquema Original ---\")\n",
    "        println(\"Columnas originales: \", names(structured_data))\n",
    "        println(\"Tipos de datos originales:\")\n",
    "        for (name, type) in zip(names(structured_data), eltype.(eachcol(structured_data)))\n",
    "            println(\"  $name: $type\")\n",
    "        end\n",
    "        println()\n",
    "        \n",
    "        # 1. Normalizar nombres de columnas (snake_case)\n",
    "        column_mapping = Dict(\n",
    "            :ID_USER => :user_id,\n",
    "            :user_name => :user_name,\n",
    "            :registration_date => :registration_date,\n",
    "            :total_spent => :total_spent,\n",
    "            :country_code => :country_code,\n",
    "            :email => :email_address\n",
    "        )\n",
    "        \n",
    "        # Solo renombrar columnas que existen\n",
    "        existing_columns = Dict(k => v for (k, v) in column_mapping if k in names(structured_data))\n",
    "        rename!(structured_data, existing_columns)\n",
    "        \n",
    "        # 2. Convertir tipos de datos\n",
    "        # Convertir fechas (en Julia usamos Date del paquete Dates)\n",
    "        structured_data.registration_date = [Date(date_str) for date_str in structured_data.registration_date]\n",
    "        \n",
    "        # Asegurar tipos numéricos\n",
    "        structured_data.user_id = Int64.(structured_data.user_id)\n",
    "        structured_data.total_spent = Float64.(structured_data.total_spent)\n",
    "        \n",
    "        # 3. Agregar metadatos de procesamiento\n",
    "        structured_data.processed_at = fill(now(), nrow(structured_data))\n",
    "        structured_data.data_source = fill(\"user_database\", nrow(structured_data))\n",
    "        \n",
    "        # 4. Reordenar columnas según esquema del data warehouse\n",
    "        column_order = [:user_id, :user_name, :email_address, :country_code, \n",
    "                       :registration_date, :total_spent, :processed_at, :data_source]\n",
    "        \n",
    "        # Solo incluir columnas que existen\n",
    "        available_columns = [col for col in column_order if col in names(structured_data)]\n",
    "        structured_data = select(structured_data, available_columns)\n",
    "        \n",
    "        println(\"--- 3. Datos Estructurados (Esquema Normalizado) ---\")\n",
    "        println(\"Columnas finales: \", names(structured_data))\n",
    "        println(structured_data)\n",
    "        println(\"\\nInformación del esquema final:\")\n",
    "        for (name, type) in zip(names(structured_data), eltype.(eachcol(structured_data)))\n",
    "            println(\"  $name: $type\")\n",
    "        end\n",
    "        println(\"\")\n",
    "        \n",
    "        @info \"Normalización completada: $(nrow(structured_data)) registros estructurados\"\n",
    "        return structured_data\n",
    "        \n",
    "    catch e\n",
    "        @error \"Error en la normalización: $e\"\n",
    "        rethrow(e)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303367a3",
   "metadata": {},
   "source": [
    "### ▶️ Ejecutemos la Normalización\n",
    "\n",
    "Ahora vamos a normalizar el esquema de nuestros datos limpios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4dc494bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mIniciando normalización del esquema...\n",
      "--- Análisis del Esquema Original ---\n",
      "Columnas originales: [\"ID_USER\", \"user_name\", \"registration_date\", \"total_spent\", \"country_code\", \"email\"]\n",
      "Tipos de datos originales:\n",
      "  ID_USER: Int64\n",
      "  user_name: String\n",
      "  registration_date: String\n",
      "  total_spent: Float64\n",
      "  country_code: String\n",
      "  email: String\n",
      "\n",
      "\u001b[91m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[91m\u001b[1mError: \u001b[22m\u001b[39mError en la normalización: ArgumentError(\"column name :user_id not found in the data frame\")\n",
      "\u001b[91m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ Main ~/Documents/ETL-ELT-EAI/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_Y104sZmlsZQ==.jl:70\u001b[39m\n"
     ]
    },
    {
     "ename": "ArgumentError",
     "evalue": "ArgumentError: column name :user_id not found in the data frame",
     "output_type": "error",
     "traceback": [
      "ArgumentError: column name :user_id not found in the data frame\n",
      "\n",
      "Stacktrace:\n",
      " [1] lookupname\n",
      "   @ ~/.julia/packages/DataFrames/kcA9R/src/other/index.jl:431 [inlined]\n",
      " [2] getindex\n",
      "   @ ~/.julia/packages/DataFrames/kcA9R/src/other/index.jl:440 [inlined]\n",
      " [3] getindex(df::DataFrame, ::typeof(!), col_ind::Symbol)\n",
      "   @ DataFrames ~/.julia/packages/DataFrames/kcA9R/src/dataframe/dataframe.jl:557\n",
      " [4] getproperty(df::DataFrame, col_ind::Symbol)\n",
      "   @ DataFrames ~/.julia/packages/DataFrames/kcA9R/src/abstractdataframe/abstractdataframe.jl:448\n",
      " [5] normalize_schema(cleaned_data::DataFrame)\n",
      "   @ Main ~/Documents/ETL-ELT-EAI/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_Y104sZmlsZQ==.jl:42\n",
      " [6] top-level scope\n",
      "   @ ~/Documents/ETL-ELT-EAI/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_Y106sZmlsZQ==.jl:3"
     ]
    }
   ],
   "source": [
    "# Ejecutar la normalización (necesitamos los datos transformados)\n",
    "if @isdefined(cleaned_data)\n",
    "    structured_data = normalize_schema(cleaned_data)\n",
    "else\n",
    "    println(\"⚠️ Primero ejecuta las celdas de extracción y transformación\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load_section",
   "metadata": {},
   "source": [
    "## 4. Load: Carga de Datos\n",
    "\n",
    "### 🎯 Objetivos de la Fase Load\n",
    "- Cargar datos en el sistema de destino\n",
    "- Mantener integridad referencial\n",
    "- Optimizar el rendimiento de carga\n",
    "- Implementar estrategias de carga incremental\n",
    "\n",
    "### 📊 Estrategias de Carga\n",
    "- **Full Load**: Carga completa de todos los datos\n",
    "- **Incremental Load**: Solo datos nuevos o modificados\n",
    "- **Upsert**: Insertar nuevos registros, actualizar existentes\n",
    "- **SCD (Slowly Changing Dimensions)**: Manejo de cambios históricos\n",
    "\n",
    "La fase final es cargar los datos transformados en el sistema de destino (Data Warehouse, Data Lake, etc.).\n",
    "\n",
    "**Ejemplo:** Simularemos diferentes tipos de carga:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "load_function",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "load_to_warehouse (generic function with 2 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function load_to_warehouse(structured_data::DataFrame, load_type=\"full\")\n",
    "    \"\"\"\n",
    "    Carga datos al data warehouse con diferentes estrategias.\n",
    "    \n",
    "    Args:\n",
    "        structured_data (DataFrame): Datos estructurados a cargar\n",
    "        load_type (String): Tipo de carga (\"full\", \"incremental\", \"upsert\")\n",
    "    \n",
    "    Returns:\n",
    "        Bool: true si la carga fue exitosa\n",
    "    \"\"\"\n",
    "    try\n",
    "        @info \"Iniciando carga de datos - Tipo: $load_type\"\n",
    "        \n",
    "        # Validaciones pre-carga\n",
    "        if nrow(structured_data) == 0\n",
    "            @warn \"No hay datos para cargar\"\n",
    "            return false\n",
    "        end\n",
    "        \n",
    "        println(\"--- Validaciones Pre-Carga ---\")\n",
    "        println(\"Registros a cargar: \", nrow(structured_data))\n",
    "        println(\"Columnas: \", names(structured_data))\n",
    "        \n",
    "        # Verificar duplicados\n",
    "        if \"user_id\" in names(structured_data)\n",
    "            duplicates = nrow(structured_data) - length(unique(structured_data.user_id))\n",
    "            if duplicates > 0\n",
    "                println(\"⚠️  Encontrados $duplicates registros duplicados\")\n",
    "                structured_data = unique(structured_data, :user_id)\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        # Simular diferentes tipos de carga\n",
    "        filename = \"\"\n",
    "        if load_type == \"full\"\n",
    "            # Carga completa - reemplaza todos los datos\n",
    "            filename = \"data_warehouse_users_full.csv\"\n",
    "            CSV.write(filename, structured_data)\n",
    "            println(\"✅ Carga completa realizada: $filename\")\n",
    "            \n",
    "        elseif load_type == \"incremental\"\n",
    "            # Carga incremental - solo nuevos registros\n",
    "            filename = \"data_warehouse_users_incremental.csv\"\n",
    "            # En un caso real, verificaríamos qué registros ya existen\n",
    "            CSV.write(filename, structured_data, append=true)\n",
    "            println(\"✅ Carga incremental realizada: $filename\")\n",
    "            \n",
    "        elseif load_type == \"upsert\"\n",
    "            # Upsert - insertar nuevos, actualizar existentes\n",
    "            filename = \"data_warehouse_users_upsert.csv\"\n",
    "            CSV.write(filename, structured_data)\n",
    "            println(\"✅ Upsert realizado: $filename\")\n",
    "        end\n",
    "        \n",
    "        println(\"\\n--- 4. Datos Cargados ---\")\n",
    "        println(\"Registros cargados exitosamente: \", nrow(structured_data))\n",
    "        println(\"\\nPrimeras filas del archivo de destino:\")\n",
    "        \n",
    "        # Mostrar contenido del archivo\n",
    "        if isfile(filename)\n",
    "            lines = readlines(filename)\n",
    "            for (i, line) in enumerate(lines[1:min(6, length(lines))])\n",
    "                println(line)\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        # Estadísticas de carga\n",
    "        println(\"\\n--- Estadísticas de Carga ---\")\n",
    "        println(\"📊 Total de registros: \", nrow(structured_data))\n",
    "        if \"country_code\" in names(structured_data)\n",
    "            println(\"📊 Países únicos: \", length(unique(structured_data.country_code)))\n",
    "        end\n",
    "        if \"total_spent\" in names(structured_data)\n",
    "            println(\"📊 Gasto promedio: \\$\", round(mean(structured_data.total_spent), digits=2))\n",
    "            println(\"📊 Gasto total: \\$\", round(sum(structured_data.total_spent), digits=2))\n",
    "        end\n",
    "        \n",
    "        @info \"Carga completada exitosamente: $(nrow(structured_data)) registros\"\n",
    "        return true\n",
    "        \n",
    "    catch e\n",
    "        @error \"Error en la carga: $e\"\n",
    "        return false\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "execute_load",
   "metadata": {},
   "source": [
    "### ▶️ Ejecutemos la Carga\n",
    "\n",
    "Ahora vamos a cargar nuestros datos estructurados al data warehouse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "load_execution",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Primero ejecuta todas las celdas anteriores\n"
     ]
    }
   ],
   "source": [
    "# Ejecutar la carga (necesitamos los datos estructurados)\n",
    "if @isdefined(structured_data)\n",
    "    success = load_to_warehouse(structured_data, \"full\")\n",
    "    println(\"\\n🎉 Carga completada: $success\")\n",
    "else\n",
    "    println(\"⚠️ Primero ejecuta todas las celdas anteriores\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complete_pipeline",
   "metadata": {},
   "source": [
    "## 🔄 Pipeline ETL Completo\n",
    "\n",
    "Ahora ejecutemos todo el pipeline de una vez:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "full_pipeline",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Iniciando Pipeline ETL Completo\n",
      "==================================================\n",
      "\n",
      "1️⃣ EXTRACCIÓN\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mIniciando extracción de datos...\n",
      "--- 1. Datos Crudos Extraídos ---\n",
      "Registros extraídos: 6\n",
      "Columnas: [\"ID_USER\", \"user_name\", \"registration_date\", \"total_spent\", \"country_code\", \"email\"]\n",
      "\n",
      "Primeras filas:\n",
      "\u001b[1m6×6 DataFrame\u001b[0m\n",
      "\u001b[1m Row \u001b[0m│\u001b[1m ID_USER \u001b[0m\u001b[1m user_name \u001b[0m\u001b[1m registration_date \u001b[0m\u001b[1m total_spent \u001b[0m\u001b[1m country_code \u001b[0m\u001b[1m email           \u001b[0m\n",
      "     │\u001b[90m Int64   \u001b[0m\u001b[90m String?   \u001b[0m\u001b[90m String            \u001b[0m\u001b[90m Any         \u001b[0m\u001b[90m String       \u001b[0m\u001b[90m String          \u001b[0m\n",
      "─────┼───────────────────────────────────────────────────────────────────────────────────\n",
      "   1 │     101  Ana        2025-01-15         150.5        ES            ana@email.com\n",
      "   2 │     102  Luis       2025-02-20         80.0         MX            luis@email.com\n",
      "   3 │     103  Marta      2025-03-01         -999.0       ES            marta@email.com\n",
      "   4 │     104  Juan       2025-04-10         200.0        CO            juan@email.com\n",
      "   5 │     105  Eva        2025-05-19         45.25        es            eva@email.com\n",
      "   6 │     106 \u001b[90m missing   \u001b[0m 2025-06-30         invalid      ES            pedro@email.com\n",
      "\n",
      "Información del DataFrame:\n",
      "Tipos de datos: Type[Int64, Union{Missing, String}, String, Any, String, String]\n",
      "\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mExtracción completada: 6 registros\n",
      "\n",
      "2️⃣ TRANSFORMACIÓN\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mIniciando transformación de datos...\n",
      "--- Análisis de Calidad de Datos ---\n",
      "Registros iniciales: 6\n",
      "Valores nulos por columna:\n",
      "  ID_USER: 0\n",
      "  user_name: 1\n",
      "  registration_date: 0\n",
      "  total_spent: 0\n",
      "  country_code: 0\n",
      "  email: 0\n",
      "\n",
      "⚠️  Encontrados 1 nombres de usuario nulos\n",
      "⚠️  Encontrados 1 valores inválidos en total_spent\n",
      "📊 Registros filtrados (gasto <= 50): 3\n",
      "📊 Registros finales: 3\n",
      "\n",
      "--- 2. Datos Limpios (Reglas Aplicadas) ---\n",
      "\u001b[1m3×6 DataFrame\u001b[0m\n",
      "\u001b[1m Row \u001b[0m│\u001b[1m ID_USER \u001b[0m\u001b[1m user_name \u001b[0m\u001b[1m registration_date \u001b[0m\u001b[1m total_spent \u001b[0m\u001b[1m country_code \u001b[0m\u001b[1m email          \u001b[0m\n",
      "     │\u001b[90m Int64   \u001b[0m\u001b[90m String    \u001b[0m\u001b[90m String            \u001b[0m\u001b[90m Float64     \u001b[0m\u001b[90m String       \u001b[0m\u001b[90m String         \u001b[0m\n",
      "─────┼──────────────────────────────────────────────────────────────────────────────────\n",
      "   1 │     101  Ana        2025-01-15               150.5  ES            ana@email.com\n",
      "   2 │     102  Luis       2025-02-20                80.0  MX            luis@email.com\n",
      "   3 │     104  Juan       2025-04-10               200.0  CO            juan@email.com\n",
      "\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mTransformación completada: 3 registros válidos\n",
      "\n",
      "3️⃣ NORMALIZACIÓN\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mIniciando normalización del esquema...\n",
      "--- Análisis del Esquema Original ---\n",
      "Columnas originales: [\"ID_USER\", \"user_name\", \"registration_date\", \"total_spent\", \"country_code\", \"email\"]\n",
      "Tipos de datos originales:\n",
      "  ID_USER: Int64\n",
      "  user_name: String\n",
      "  registration_date: String\n",
      "  total_spent: Float64\n",
      "  country_code: String\n",
      "  email: String\n",
      "\n",
      "\u001b[91m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[91m\u001b[1mError: \u001b[22m\u001b[39mError en la normalización: ArgumentError(\"column name :user_id not found in the data frame\")\n",
      "\u001b[91m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ Main ~/Documents/ETL-ELT-EAI/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_Y104sZmlsZQ==.jl:70\u001b[39m\n",
      "\u001b[91m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[91m\u001b[1mError: \u001b[22m\u001b[39mError en el pipeline ETL: ArgumentError(\"column name :user_id not found in the data frame\")\n",
      "\u001b[91m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ Main ~/Documents/ETL-ELT-EAI/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_Y115sZmlsZQ==.jl:39\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "function run_complete_etl_pipeline()\n",
    "    \"\"\"\n",
    "    Ejecuta el pipeline ETL completo de principio a fin.\n",
    "    \"\"\"\n",
    "    println(\"🚀 Iniciando Pipeline ETL Completo\")\n",
    "    println(\"=\" ^ 50)\n",
    "    \n",
    "    try\n",
    "        # 1. Extract\n",
    "        println(\"\\n1️⃣ EXTRACCIÓN\")\n",
    "        raw_data = extract_from_database()\n",
    "        \n",
    "        # 2. Transform\n",
    "        println(\"\\n2️⃣ TRANSFORMACIÓN\")\n",
    "        cleaned_data = apply_business_rules(raw_data)\n",
    "        \n",
    "        # 3. Normalize\n",
    "        println(\"\\n3️⃣ NORMALIZACIÓN\")\n",
    "        structured_data = normalize_schema(cleaned_data)\n",
    "        \n",
    "        # 4. Load\n",
    "        println(\"\\n4️⃣ CARGA\")\n",
    "        success = load_to_warehouse(structured_data, \"full\")\n",
    "        \n",
    "        if success\n",
    "            println(\"\\n🎉 ¡PIPELINE ETL COMPLETADO EXITOSAMENTE!\")\n",
    "            println(\"=\" ^ 50)\n",
    "            println(\"📊 Resumen Final:\")\n",
    "            println(\"   • Registros procesados: $(nrow(structured_data))\")\n",
    "            println(\"   • Columnas finales: $(length(names(structured_data)))\")\n",
    "            println(\"   • Archivo generado: data_warehouse_users_full.csv\")\n",
    "        else\n",
    "            println(\"❌ Error en el pipeline\")\n",
    "        end\n",
    "        \n",
    "        return structured_data\n",
    "        \n",
    "    catch e\n",
    "        @error \"Error en el pipeline ETL: $e\"\n",
    "        return nothing\n",
    "    end\n",
    "end\n",
    "\n",
    "# Ejecutar el pipeline completo\n",
    "final_data = run_complete_etl_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monitoring_section",
   "metadata": {},
   "source": [
    "## 📊 Monitoreo y Calidad de Datos\n",
    "\n",
    "### 🔍 Validaciones de Calidad\n",
    "\n",
    "Es crucial monitorear la calidad de los datos en cada etapa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "data_quality",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📋 REPORTE DE CALIDAD - EXTRACCIÓN\n",
      "========================================\n",
      "📊 Estadísticas Básicas:\n",
      "   • Total de registros: 6\n",
      "   • Total de columnas: 6\n",
      "   • Columnas: ID_USER, user_name, registration_date, total_spent, country_code, email\n",
      "\n",
      "🔍 Valores Faltantes:\n",
      "   • user_name: 1 (16.67%)\n",
      "\n",
      "✅ Validaciones Específicas:\n",
      "   • Emails válidos: 6/6\n"
     ]
    },
    {
     "ename": "MethodError",
     "evalue": "MethodError: no method matching isless(::Int64, ::String)\nThe function `isless` exists, but no method is defined for this combination of argument types.\n\nClosest candidates are:\n  isless(!Matched::Missing, ::Any)\n   @ Base missing.jl:87\n  isless(::Any, !Matched::Missing)\n   @ Base missing.jl:88\n  isless(::Real, !Matched::AbstractFloat)\n   @ Base operators.jl:179\n  ...\n",
     "output_type": "error",
     "traceback": [
      "MethodError: no method matching isless(::Int64, ::String)\n",
      "The function `isless` exists, but no method is defined for this combination of argument types.\n",
      "\n",
      "Closest candidates are:\n",
      "  isless(!Matched::Missing, ::Any)\n",
      "   @ Base missing.jl:87\n",
      "  isless(::Any, !Matched::Missing)\n",
      "   @ Base missing.jl:88\n",
      "  isless(::Real, !Matched::AbstractFloat)\n",
      "   @ Base operators.jl:179\n",
      "  ...\n",
      "\n",
      "\n",
      "Stacktrace:\n",
      "  [1] <(x::Int64, y::String)\n",
      "    @ Base ./operators.jl:353\n",
      "  [2] <=(x::Int64, y::String)\n",
      "    @ Base ./operators.jl:402\n",
      "  [3] >=(x::String, y::Int64)\n",
      "    @ Base ./operators.jl:426\n",
      "  [4] _broadcast_getindex_evalf\n",
      "    @ ./broadcast.jl:678 [inlined]\n",
      "  [5] _broadcast_getindex\n",
      "    @ ./broadcast.jl:651 [inlined]\n",
      "  [6] getindex\n",
      "    @ ./broadcast.jl:610 [inlined]\n",
      "  [7] copyto_nonleaf!(dest::BitVector, bc::Base.Broadcast.Broadcasted{Base.Broadcast.DefaultArrayStyle{1}, Tuple{Base.OneTo{Int64}}, typeof(>=), Tuple{Base.Broadcast.Extruded{Vector{Any}, Tuple{Bool}, Tuple{Int64}}, Int64}}, iter::Base.OneTo{Int64}, state::Int64, count::Int64)\n",
      "    @ Base.Broadcast ./broadcast.jl:1082\n",
      "  [8] copy\n",
      "    @ ./broadcast.jl:919 [inlined]\n",
      "  [9] materialize(bc::Base.Broadcast.Broadcasted{Base.Broadcast.DefaultArrayStyle{1}, Nothing, typeof(>=), Tuple{Vector{Any}, Int64}})\n",
      "    @ Base.Broadcast ./broadcast.jl:872\n",
      " [10] data_quality_report(df::DataFrame, stage_name::String)\n",
      "    @ Main ~/Documents/ETL-ELT-EAI/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_Y120sZmlsZQ==.jl:50\n",
      " [11] top-level scope\n",
      "    @ ~/Documents/ETL-ELT-EAI/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_Y120sZmlsZQ==.jl:59"
     ]
    }
   ],
   "source": [
    "function data_quality_report(df::DataFrame, stage_name::String)\n",
    "    \"\"\"\n",
    "    Genera un reporte de calidad de datos.\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): DataFrame a analizar\n",
    "        stage_name (String): Nombre de la etapa del pipeline\n",
    "    \"\"\"\n",
    "    println(\"\\n📋 REPORTE DE CALIDAD - $stage_name\")\n",
    "    println(\"=\" ^ 40)\n",
    "    \n",
    "    # Estadísticas básicas\n",
    "    println(\"📊 Estadísticas Básicas:\")\n",
    "    println(\"   • Total de registros: $(nrow(df))\")\n",
    "    println(\"   • Total de columnas: $(ncol(df))\")\n",
    "    println(\"   • Columnas: $(join(names(df), \", \"))\")\n",
    "    \n",
    "    # Valores faltantes\n",
    "    println(\"\\n🔍 Valores Faltantes:\")\n",
    "    for col in names(df)\n",
    "        missing_count = sum(ismissing.(df[!, col]))\n",
    "        missing_pct = round(missing_count / nrow(df) * 100, digits=2)\n",
    "        if missing_count > 0\n",
    "            println(\"   • $col: $missing_count ($missing_pct%)\")\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    # Duplicados\n",
    "    if \"user_id\" in names(df)\n",
    "        duplicates = nrow(df) - length(unique(df.user_id))\n",
    "        println(\"\\n🔄 Duplicados:\")\n",
    "        println(\"   • Registros duplicados: $duplicates\")\n",
    "    end\n",
    "    \n",
    "    # Validaciones específicas por columna\n",
    "    println(\"\\n✅ Validaciones Específicas:\")\n",
    "    \n",
    "    if \"email\" in names(df) || \"email_address\" in names(df)\n",
    "        email_col = \"email\" in names(df) ? \"email\" : \"email_address\"\n",
    "        valid_emails = sum(occursin.(r\"@\", df[!, email_col]))\n",
    "        println(\"   • Emails válidos: $valid_emails/$(nrow(df))\")\n",
    "    end\n",
    "    \n",
    "    if \"age\" in names(df)\n",
    "        valid_ages = sum((df.age .>= 0) .& (df.age .<= 120))\n",
    "        println(\"   • Edades válidas (0-120): $valid_ages/$(nrow(df))\")\n",
    "    end\n",
    "    \n",
    "    if \"total_spent\" in names(df)\n",
    "        valid_amounts = sum(df.total_spent .>= 0)\n",
    "        println(\"   • Montos válidos (>=0): $valid_amounts/$(nrow(df))\")\n",
    "    end\n",
    "    \n",
    "    println(\"\\n\" * \"=\" ^ 40)\n",
    "end\n",
    "\n",
    "# Generar reportes de calidad para cada etapa\n",
    "if @isdefined(raw_data)\n",
    "    data_quality_report(raw_data, \"EXTRACCIÓN\")\n",
    "end\n",
    "\n",
    "if @isdefined(cleaned_data)\n",
    "    data_quality_report(cleaned_data, \"TRANSFORMACIÓN\")\n",
    "end\n",
    "\n",
    "if @isdefined(structured_data)\n",
    "    data_quality_report(structured_data, \"NORMALIZACIÓN\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "best_practices",
   "metadata": {},
   "source": [
    "## 🎯 Mejores Prácticas ETL en Julia\n",
    "\n",
    "### 📋 Checklist de Mejores Prácticas\n",
    "\n",
    "#### ✅ Diseño del Pipeline\n",
    "- **Modularidad**: Cada función tiene una responsabilidad específica\n",
    "- **Reutilización**: Funciones genéricas que se pueden usar en múltiples pipelines\n",
    "- **Configurabilidad**: Parámetros externos para diferentes entornos\n",
    "\n",
    "#### ✅ Manejo de Errores\n",
    "- **Try-catch**: Captura y manejo de excepciones\n",
    "- **Logging**: Uso de `@info`, `@warn`, `@error` para trazabilidad\n",
    "- **Validaciones**: Verificación de datos en cada etapa\n",
    "\n",
    "#### ✅ Performance\n",
    "- **Broadcasting**: Uso de `.` para operaciones vectorizadas\n",
    "- **Memory efficiency**: Evitar copias innecesarias de DataFrames\n",
    "- **Parallel processing**: Usar `@threads` para operaciones paralelas\n",
    "\n",
    "#### ✅ Calidad de Datos\n",
    "- **Validaciones**: Verificar integridad en cada paso\n",
    "- **Monitoreo**: Reportes de calidad automáticos\n",
    "- **Documentación**: Funciones bien documentadas\n",
    "\n",
    "### 🔧 Herramientas Complementarias\n",
    "\n",
    "#### Paquetes Julia para ETL:\n",
    "- **DataFrames.jl**: Manipulación de datos tabulares\n",
    "- **CSV.jl**: Lectura/escritura de archivos CSV\n",
    "- **JSON3.jl**: Manejo de datos JSON\n",
    "- **HTTP.jl**: Consumo de APIs REST\n",
    "- **Dates.jl**: Manipulación de fechas\n",
    "- **Statistics.jl**: Estadísticas descriptivas\n",
    "- **Query.jl**: Consultas tipo LINQ\n",
    "- **MLJ.jl**: Machine Learning (para pipelines ML)\n",
    "\n",
    "#### Bases de Datos:\n",
    "- **LibPQ.jl**: PostgreSQL\n",
    "- **MySQL.jl**: MySQL\n",
    "- **SQLite.jl**: SQLite\n",
    "- **ODBC.jl**: Conexiones ODBC genéricas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercises",
   "metadata": {},
   "source": [
    "## 🏋️ Ejercicios Prácticos\n",
    "\n",
    "### Ejercicio 1: Pipeline de Ventas\n",
    "Crea un pipeline ETL para procesar datos de ventas con las siguientes características:\n",
    "- Extraer datos de múltiples archivos CSV\n",
    "- Calcular métricas de ventas por región\n",
    "- Detectar transacciones anómalas\n",
    "- Generar reporte consolidado\n",
    "\n",
    "### Ejercicio 2: Pipeline de Logs\n",
    "Desarrolla un pipeline para procesar logs de aplicación:\n",
    "- Parsear logs en formato no estructurado\n",
    "- Extraer información de errores\n",
    "- Calcular métricas de performance\n",
    "- Alertas automáticas\n",
    "\n",
    "### Ejercicio 3: Pipeline de Redes Sociales\n",
    "Construye un pipeline para analizar datos de redes sociales:\n",
    "- Consumir API de Twitter/Facebook\n",
    "- Análisis de sentimientos\n",
    "- Detección de tendencias\n",
    "- Dashboard en tiempo real\n",
    "\n",
    "### 💡 Desafío Avanzado\n",
    "Implementa un pipeline ETL que:\n",
    "1. Procese datos en tiempo real usando streaming\n",
    "2. Implemente machine learning para detección de anomalías\n",
    "3. Use paralelización para mejorar performance\n",
    "4. Incluya tests automatizados\n",
    "5. Tenga monitoreo y alertas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resources",
   "metadata": {},
   "source": [
    "## 📚 Recursos Adicionales\n",
    "\n",
    "### 📖 Documentación Oficial\n",
    "- [Julia Documentation](https://docs.julialang.org/)\n",
    "- [DataFrames.jl Guide](https://dataframes.juliadata.org/stable/)\n",
    "- [CSV.jl Documentation](https://csv.juliadata.org/stable/)\n",
    "\n",
    "### 🎓 Cursos y Tutoriales\n",
    "- [Julia Academy](https://juliaacademy.com/)\n",
    "- [Think Julia Book](https://benlauwens.github.io/ThinkJulia.jl/latest/book.html)\n",
    "- [Julia Data Science](https://juliadatascience.io/)\n",
    "\n",
    "### 🛠️ Herramientas y Frameworks\n",
    "- [Pluto.jl](https://github.com/fonsp/Pluto.jl) - Notebooks reactivos\n",
    "- [Genie.jl](https://genieframework.com/) - Framework web\n",
    "- [MLJ.jl](https://alan-turing-institute.github.io/MLJ.jl/dev/) - Machine Learning\n",
    "- [Flux.jl](https://fluxml.ai/) - Deep Learning\n",
    "\n",
    "### 🌐 Comunidad\n",
    "- [Julia Discourse](https://discourse.julialang.org/)\n",
    "- [Julia Slack](https://julialang.org/slack/)\n",
    "- [JuliaCon](https://juliacon.org/) - Conferencia anual\n",
    "\n",
    "### 📊 Casos de Uso Reales\n",
    "- **Finanzas**: Análisis de riesgo, trading algorítmico\n",
    "- **Ciencia**: Simulaciones científicas, bioinformática\n",
    "- **ML/AI**: Modelos de machine learning, deep learning\n",
    "- **Data Engineering**: Pipelines ETL, procesamiento distribuido\n",
    "\n",
    "---\n",
    "\n",
    "## 🎉 ¡Felicitaciones!\n",
    "\n",
    "Has completado el tutorial de ETL con Julia. Ahora tienes las herramientas para:\n",
    "\n",
    "✅ Construir pipelines ETL robustos y escalables  \n",
    "✅ Manejar datos de diferentes fuentes y formatos  \n",
    "✅ Implementar validaciones y monitoreo de calidad  \n",
    "✅ Aplicar mejores prácticas de ingeniería de datos  \n",
    "✅ Usar las características únicas de Julia para performance  \n",
    "\n",
    "**¡Sigue practicando y construyendo pipelines cada vez más sofisticados!** 🚀"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.6",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

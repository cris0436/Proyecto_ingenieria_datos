{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53cd153d",
   "metadata": {},
   "source": [
    "# Proceso ETL: Una Guía Práctica con Python\n",
    "\n",
    "## 📚 Objetivos de Aprendizaje\n",
    "\n",
    "Al completar este notebook, los estudiantes serán capaces de:\n",
    "- Comprender los conceptos fundamentales de ETL (Extract, Transform, Load)\n",
    "- Implementar cada fase del proceso ETL usando Python y pandas\n",
    "- Aplicar técnicas de limpieza y transformación de datos\n",
    "- Manejar errores comunes en pipelines de datos\n",
    "- Diseñar pipelines ETL escalables y mantenibles\n",
    "\n",
    "## 🎯 ¿Qué es ETL?\n",
    "\n",
    "**ETL** es un proceso fundamental en ingeniería de datos que permite:\n",
    "- **Extract (Extraer)**: Obtener datos de múltiples fuentes\n",
    "- **Transform (Transformar)**: Limpiar, validar y estructurar los datos\n",
    "- **Load (Cargar)**: Almacenar los datos procesados en el destino final\n",
    "\n",
    "Este notebook es una guía completa que cubre desde conceptos básicos hasta temas avanzados, con ejemplos prácticos y ejercicios para trabajar con pipelines ETL en entornos reales de ingeniería de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ec1edb",
   "metadata": {},
   "source": [
    "## El Pipeline ETL de Referencia\n",
    "\n",
    "Nos basaremos en la siguiente estructura de pipeline, que representa un flujo ETL clásico:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a2ae36c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def etl_pipeline():\n",
    "    # 1. Extraer datos desde una fuente original\n",
    "    raw_data = extract_from_database()\n",
    "    \n",
    "    # 2. Transformar los datos aplicando reglas y limpiando\n",
    "    cleaned_data = apply_business_rules(raw_data)\n",
    "    \n",
    "    # 3. Normalizar el esquema para que sea consistente\n",
    "    structured_data = normalize_schema(cleaned_data)\n",
    "    \n",
    "    # 4. Cargar los datos transformados a su destino final\n",
    "    load_to_warehouse(structured_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b297d4",
   "metadata": {},
   "source": [
    "A continuación, implementaremos cada una de estas funciones con ejemplos claros."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e9b2f4",
   "metadata": {},
   "source": [
    "## 📦 Librerías y Configuración Inicial\n",
    "\n",
    "Para nuestros ejemplos, usaremos las siguientes librerías esenciales en ingeniería de datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c3457b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd          # Manipulación y análisis de datos\n",
    "import sqlite3              # Base de datos SQL ligera\n",
    "import numpy as np          # Operaciones numéricas\n",
    "import json                 # Manejo de datos JSON\n",
    "import logging              # Sistema de logs\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuración de logging para monitorear el pipeline\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3c7036",
   "metadata": {},
   "source": [
    "## 🧠 Conceptos Fundamentales de ETL\n",
    "\n",
    "### Tipos de Procesamiento ETL\n",
    "\n",
    "1. **Batch Processing (Procesamiento por lotes)**\n",
    "   - Procesa grandes volúmenes de datos en intervalos programados\n",
    "   - Ideal para reportes diarios, semanales o mensuales\n",
    "   - Ejemplo: Procesar todas las transacciones del día anterior\n",
    "\n",
    "2. **Real-time Processing (Procesamiento en tiempo real)**\n",
    "   - Procesa datos tan pronto como llegan\n",
    "   - Crítico para aplicaciones que requieren respuesta inmediata\n",
    "   - Ejemplo: Detección de fraude en transacciones\n",
    "\n",
    "3. **Near Real-time Processing (Procesamiento casi en tiempo real)**\n",
    "   - Procesa datos con un pequeño retraso (segundos o minutos)\n",
    "   - Balance entre velocidad y eficiencia de recursos\n",
    "   - Ejemplo: Actualización de dashboards cada 5 minutos\n",
    "\n",
    "### Calidad de Datos - Las 6 Dimensiones\n",
    "\n",
    "1. **Exactitud**: Los datos reflejan la realidad\n",
    "2. **Completitud**: No hay valores faltantes críticos\n",
    "3. **Consistencia**: Los datos son coherentes entre sistemas\n",
    "4. **Validez**: Los datos cumplen con reglas de negocio\n",
    "5. **Unicidad**: No hay duplicados no deseados\n",
    "6. **Actualidad**: Los datos están actualizados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae53585",
   "metadata": {},
   "source": [
    "## 1. Extract: Extracción de Datos\n",
    "\n",
    "### 🎯 Objetivos de la Fase Extract\n",
    "- Conectar con múltiples fuentes de datos\n",
    "- Extraer datos de manera eficiente\n",
    "- Manejar diferentes formatos y protocolos\n",
    "- Implementar estrategias de extracción incremental\n",
    "\n",
    "### 📊 Fuentes de Datos Comunes\n",
    "- **Bases de datos relacionales**: MySQL, PostgreSQL, SQL Server\n",
    "- **Bases de datos NoSQL**: MongoDB, Cassandra, Redis\n",
    "- **Archivos**: CSV, JSON, XML, Parquet\n",
    "- **APIs REST**: Servicios web y microservicios\n",
    "- **Sistemas de streaming**: Kafka, Kinesis\n",
    "\n",
    "**Ejemplo:** Vamos a simular la extracción de datos de una base de datos SQL. Crearemos una pequeña base de datos en memoria con `sqlite3` y luego la leeremos usando `pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f1bcdc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_database():\n",
    "    \"\"\"\n",
    "    Simula la extracción de datos desde una base de datos.\n",
    "    En un entorno real, esto se conectaría a una base de datos real.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Datos crudos extraídos\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(\"Iniciando extracción de datos...\")\n",
    "        \n",
    "        # Simulamos datos con problemas típicos de calidad\n",
    "        data = {\n",
    "            'ID_USER': [101, 102, 103, 104, 105, 106],\n",
    "            'user_name': ['Ana', 'Luis', 'Marta', 'Juan', 'Eva', None],  # Valor nulo\n",
    "            'registration_date': ['2025-01-15', '2025-02-20', '2025-03-01', '2025-04-10', '2025-05-19', '2025-06-30'],\n",
    "            'total_spent': [150.5, 80.0, -999, 200.0, 45.25, 'invalid'],  # Valor inválido\n",
    "            'country_code': ['ES', 'MX', 'ES', 'CO', 'es', 'ES'],  # Inconsistencia en mayúsculas\n",
    "            'email': ['ana@email.com', 'luis@email.com', 'marta@email.com', 'juan@email.com', 'eva@email.com', 'pedro@email.com']\n",
    "        }\n",
    "        \n",
    "        df = pd.DataFrame(data)\n",
    "        \n",
    "        print(\"--- 1. Datos Crudos Extraídos ---\")\n",
    "        print(f\"Registros extraídos: {len(df)}\")\n",
    "        print(f\"Columnas: {list(df.columns)}\")\n",
    "        print(\"\\nPrimeras filas:\")\n",
    "        print(df)\n",
    "        print(\"\\nInformación del DataFrame:\")\n",
    "        print(df.info())\n",
    "        print('')\n",
    "        \n",
    "        logger.info(f\"Extracción completada: {len(df)} registros\")\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error en la extracción: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d280cc5e",
   "metadata": {},
   "source": [
    "### ▶️ Ejecutemos la Extracción\n",
    "\n",
    "Ahora vamos a ejecutar la función de extracción para ver los datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0f50a460",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-31 15:46:34,049 - INFO - Iniciando extracción de datos...\n",
      "2025-08-31 15:46:34,052 - INFO - Extracción completada: 6 registros\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Datos Crudos Extraídos ---\n",
      "Registros extraídos: 6\n",
      "Columnas: ['ID_USER', 'user_name', 'registration_date', 'total_spent', 'country_code', 'email']\n",
      "\n",
      "Primeras filas:\n",
      "   ID_USER user_name registration_date total_spent country_code  \\\n",
      "0      101       Ana        2025-01-15       150.5           ES   \n",
      "1      102      Luis        2025-02-20        80.0           MX   \n",
      "2      103     Marta        2025-03-01        -999           ES   \n",
      "3      104      Juan        2025-04-10       200.0           CO   \n",
      "4      105       Eva        2025-05-19       45.25           es   \n",
      "5      106      None        2025-06-30     invalid           ES   \n",
      "\n",
      "             email  \n",
      "0    ana@email.com  \n",
      "1   luis@email.com  \n",
      "2  marta@email.com  \n",
      "3   juan@email.com  \n",
      "4    eva@email.com  \n",
      "5  pedro@email.com  \n",
      "\n",
      "Información del DataFrame:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6 entries, 0 to 5\n",
      "Data columns (total 6 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   ID_USER            6 non-null      int64 \n",
      " 1   user_name          5 non-null      object\n",
      " 2   registration_date  6 non-null      object\n",
      " 3   total_spent        6 non-null      object\n",
      " 4   country_code       6 non-null      object\n",
      " 5   email              6 non-null      object\n",
      "dtypes: int64(1), object(5)\n",
      "memory usage: 420.0+ bytes\n",
      "None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ejecutar la función de extracción\n",
    "raw_data = extract_from_database()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac2fd69",
   "metadata": {},
   "source": [
    "### 📁 Ejemplo: Extracción desde Archivo CSV\n",
    "\n",
    "Veamos cómo extraer datos desde un archivo CSV (otra fuente común):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fcbfca62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Extracción desde CSV ---\n",
      "   product_id product_name   price     category\n",
      "0           1       Laptop  999.99  Electronics\n",
      "1           2        Mouse   25.50  Accessories\n",
      "2           3     Keyboard   75.00  Accessories\n",
      "3           4      Monitor  299.99  Electronics\n",
      "4           5   Headphones   89.99  Accessories\n"
     ]
    }
   ],
   "source": [
    "def extract_from_csv(file_path='sample_products.csv'):\n",
    "    \"\"\"\n",
    "    Extrae datos desde un archivo CSV.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Ruta del archivo CSV\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Datos extraídos del CSV\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Crear un archivo CSV de ejemplo\n",
    "        sample_data = {\n",
    "            'product_id': [1, 2, 3, 4, 5],\n",
    "            'product_name': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Headphones'],\n",
    "            'price': [999.99, 25.50, 75.00, 299.99, 89.99],\n",
    "            'category': ['Electronics', 'Accessories', 'Accessories', 'Electronics', 'Accessories']\n",
    "        }\n",
    "        \n",
    "        df_sample = pd.DataFrame(sample_data)\n",
    "        df_sample.to_csv(file_path, index=False)\n",
    "        \n",
    "        # Leer el archivo CSV\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        print(\"--- Extracción desde CSV ---\")\n",
    "        print(df)\n",
    "        return df\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"Archivo no encontrado: {file_path}\")\n",
    "        return pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error leyendo CSV: {str(e)}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Ejemplo de uso\n",
    "products_df = extract_from_csv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cde5da4",
   "metadata": {},
   "source": [
    "## 2. Transform: Aplicación de Reglas de Negocio\n",
    "\n",
    "### 🎯 Objetivos de la Fase Transform\n",
    "- Limpiar datos inconsistentes o erróneos\n",
    "- Validar que los datos cumplan reglas de negocio\n",
    "- Enriquecer datos con información adicional\n",
    "- Aplicar transformaciones matemáticas o lógicas\n",
    "- Normalizar formatos y estructuras\n",
    "\n",
    "### 🔧 Técnicas Comunes de Transformación\n",
    "- **Limpieza**: Eliminar duplicados, corregir errores tipográficos\n",
    "- **Validación**: Verificar rangos, formatos, tipos de datos\n",
    "- **Enriquecimiento**: Agregar datos calculados o de referencia\n",
    "- **Normalización**: Estandarizar formatos y escalas\n",
    "- **Agregación**: Resumir datos por grupos o períodos\n",
    "\n",
    "**Ejemplo:** Aplicaremos las siguientes reglas:\n",
    "1. Corregir valores erróneos: El valor `-999` en `total_spent` debe ser 0\n",
    "2. Corregir inconsistencias: El código de país `es` debe ser `ES`\n",
    "3. Filtrar datos: Solo usuarios con gasto mayor a 50\n",
    "4. Manejar valores nulos en nombres de usuario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "50ebf8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_business_rules(raw_data):\n",
    "    \"\"\"\n",
    "    Aplica reglas de negocio y limpieza de datos.\n",
    "    \n",
    "    Args:\n",
    "        raw_data (pd.DataFrame): Datos crudos a transformar\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Datos limpios y transformados\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(\"Iniciando transformación de datos...\")\n",
    "        cleaned_data = raw_data.copy()\n",
    "        \n",
    "        print(\"--- Análisis de Calidad de Datos ---\")\n",
    "        print(f\"Registros iniciales: {len(cleaned_data)}\")\n",
    "        print(f\"Valores nulos por columna:\")\n",
    "        print(cleaned_data.isnull().sum())\n",
    "        print()\n",
    "        \n",
    "        # 1. Manejar valores nulos en user_name\n",
    "        null_names = cleaned_data['user_name'].isnull().sum()\n",
    "        if null_names > 0:\n",
    "            print(f\"⚠️  Encontrados {null_names} nombres de usuario nulos\")\n",
    "            cleaned_data['user_name'] = cleaned_data['user_name'].fillna('Usuario_Desconocido')\n",
    "        \n",
    "        # 2. Corregir valores erróneos en total_spent\n",
    "        # Convertir valores no numéricos a NaN, luego reemplazar -999 por 0\n",
    "        cleaned_data['total_spent'] = pd.to_numeric(cleaned_data['total_spent'], errors='coerce')\n",
    "        invalid_spent = cleaned_data['total_spent'].isnull().sum()\n",
    "        if invalid_spent > 0:\n",
    "            print(f\"⚠️  Encontrados {invalid_spent} valores inválidos en total_spent\")\n",
    "            cleaned_data['total_spent'] = cleaned_data['total_spent'].fillna(0)\n",
    "        \n",
    "        cleaned_data['total_spent'] = cleaned_data['total_spent'].replace(-999, 0)\n",
    "        \n",
    "        # 3. Normalizar códigos de país\n",
    "        cleaned_data['country_code'] = cleaned_data['country_code'].str.upper()\n",
    "        \n",
    "        # 4. Aplicar regla de negocio: solo usuarios con gasto > 50\n",
    "        initial_count = len(cleaned_data)\n",
    "        cleaned_data = cleaned_data[cleaned_data['total_spent'] > 50]\n",
    "        filtered_count = initial_count - len(cleaned_data)\n",
    "        \n",
    "        print(f\"📊 Registros filtrados (gasto <= 50): {filtered_count}\")\n",
    "        print(f\"📊 Registros finales: {len(cleaned_data)}\")\n",
    "        \n",
    "        print(\"\\n--- 2. Datos Limpios (Reglas Aplicadas) ---\")\n",
    "        print(cleaned_data)\n",
    "        print('')\n",
    "        \n",
    "        logger.info(f\"Transformación completada: {len(cleaned_data)} registros válidos\")\n",
    "        return cleaned_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error en la transformación: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab0c9ff",
   "metadata": {},
   "source": [
    "### 🔍 Técnicas Avanzadas de Transformación\n",
    "\n",
    "Veamos algunas técnicas adicionales de transformación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fbfe0693",
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_transformations(df):\n",
    "    \"\"\"\n",
    "    Aplica transformaciones avanzadas a los datos.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame a transformar\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame con transformaciones aplicadas\n",
    "    \"\"\"\n",
    "    df_transformed = df.copy()\n",
    "    \n",
    "    # 1. Crear columnas derivadas\n",
    "    df_transformed['registration_year'] = pd.to_datetime(df_transformed['registration_date']).dt.year\n",
    "    df_transformed['registration_month'] = pd.to_datetime(df_transformed['registration_date']).dt.month\n",
    "    \n",
    "    # 2. Categorizar gastos\n",
    "    def categorize_spending(amount):\n",
    "        if amount < 100:\n",
    "            return 'Bajo'\n",
    "        elif amount < 200:\n",
    "            return 'Medio'\n",
    "        else:\n",
    "            return 'Alto'\n",
    "    \n",
    "    df_transformed['spending_category'] = df_transformed['total_spent'].apply(categorize_spending)\n",
    "    \n",
    "    # 3. Validar emails (ejemplo básico)\n",
    "    if 'email' in df_transformed.columns:\n",
    "        df_transformed['email_valid'] = df_transformed['email'].str.contains('@', na=False)\n",
    "    \n",
    "    print(\"--- Transformaciones Avanzadas ---\")\n",
    "    print(df_transformed[['user_name', 'total_spent', 'spending_category', 'registration_year']].head())\n",
    "    \n",
    "    return df_transformed\n",
    "\n",
    "# Esta función se puede usar después de apply_business_rules\n",
    "# df_advanced = advanced_transformations(cleaned_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106b8a6f",
   "metadata": {},
   "source": [
    "## 3. Transform: Normalización del Esquema\n",
    "\n",
    "### 🎯 Objetivos de la Normalización\n",
    "- Estandarizar nombres de columnas\n",
    "- Asegurar tipos de datos correctos\n",
    "- Mantener consistencia entre sistemas\n",
    "- Facilitar la integración con el destino\n",
    "\n",
    "El objetivo es asegurar que el esquema de los datos (nombres de columnas, tipos de datos) sea consistente.\n",
    "\n",
    "**Ejemplo:** Vamos a:\n",
    "1. Cambiar nombres de columnas a un formato estándar (snake_case)\n",
    "2. Asegurar que `registration_date` sea de tipo fecha\n",
    "3. Reordenar columnas según el esquema del destino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b9d150fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_schema(cleaned_data):\n",
    "    \"\"\"\n",
    "    Normaliza el esquema de datos según estándares del data warehouse.\n",
    "    \n",
    "    Args:\n",
    "        cleaned_data (pd.DataFrame): Datos limpios a normalizar\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Datos con esquema normalizado\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(\"Iniciando normalización del esquema...\")\n",
    "        structured_data = cleaned_data.copy()\n",
    "        \n",
    "        print(\"--- Análisis del Esquema Original ---\")\n",
    "        print(f\"Columnas originales: {list(structured_data.columns)}\")\n",
    "        print(f\"Tipos de datos originales:\")\n",
    "        print(structured_data.dtypes)\n",
    "        print()\n",
    "        \n",
    "        # 1. Normalizar nombres de columnas (snake_case)\n",
    "        column_mapping = {\n",
    "            'ID_USER': 'user_id',\n",
    "            'user_name': 'user_name',\n",
    "            'registration_date': 'registration_date',\n",
    "            'total_spent': 'total_spent',\n",
    "            'country_code': 'country_code',\n",
    "            'email': 'email_address'\n",
    "        }\n",
    "        \n",
    "        # Solo renombrar columnas que existen\n",
    "        existing_columns = {k: v for k, v in column_mapping.items() if k in structured_data.columns}\n",
    "        structured_data = structured_data.rename(columns=existing_columns)\n",
    "        \n",
    "        # 2. Convertir tipos de datos\n",
    "        structured_data['registration_date'] = pd.to_datetime(structured_data['registration_date'])\n",
    "        structured_data['user_id'] = structured_data['user_id'].astype('int64')\n",
    "        structured_data['total_spent'] = structured_data['total_spent'].astype('float64')\n",
    "        \n",
    "        # 3. Agregar metadatos de procesamiento\n",
    "        structured_data['processed_at'] = datetime.now()\n",
    "        structured_data['data_source'] = 'user_database'\n",
    "        \n",
    "        # 4. Reordenar columnas según esquema del data warehouse\n",
    "        column_order = ['user_id', 'user_name', 'email_address', 'country_code', \n",
    "                       'registration_date', 'total_spent', 'processed_at', 'data_source']\n",
    "        \n",
    "        # Solo incluir columnas que existen\n",
    "        available_columns = [col for col in column_order if col in structured_data.columns]\n",
    "        structured_data = structured_data[available_columns]\n",
    "        \n",
    "        print(\"--- 3. Datos Estructurados (Esquema Normalizado) ---\")\n",
    "        print(f\"Columnas finales: {list(structured_data.columns)}\")\n",
    "        print(structured_data)\n",
    "        print(\"\\nInformación del esquema final:\")\n",
    "        print(structured_data.info())\n",
    "        print('')\n",
    "        \n",
    "        logger.info(f\"Normalización completada: {len(structured_data)} registros estructurados\")\n",
    "        return structured_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error en la normalización: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b95624f",
   "metadata": {},
   "source": [
    "## 4. Load: Carga de Datos\n",
    "\n",
    "### 🎯 Objetivos de la Fase Load\n",
    "- Cargar datos en el sistema de destino\n",
    "- Mantener integridad referencial\n",
    "- Optimizar el rendimiento de carga\n",
    "- Implementar estrategias de carga incremental\n",
    "\n",
    "### 📊 Estrategias de Carga\n",
    "- **Full Load**: Carga completa de todos los datos\n",
    "- **Incremental Load**: Solo datos nuevos o modificados\n",
    "- **Upsert**: Insertar nuevos registros, actualizar existentes\n",
    "- **SCD (Slowly Changing Dimensions)**: Manejo de cambios históricos\n",
    "\n",
    "La fase final es cargar los datos transformados en el sistema de destino (Data Warehouse, Data Lake, etc.).\n",
    "\n",
    "**Ejemplo:** Simularemos diferentes tipos de carga:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "55af4e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_to_warehouse(structured_data, load_type='full'):\n",
    "    \"\"\"\n",
    "    Carga datos al data warehouse con diferentes estrategias.\n",
    "    \n",
    "    Args:\n",
    "        structured_data (pd.DataFrame): Datos estructurados a cargar\n",
    "        load_type (str): Tipo de carga ('full', 'incremental', 'upsert')\n",
    "    \n",
    "    Returns:\n",
    "        bool: True si la carga fue exitosa\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Iniciando carga de datos - Tipo: {load_type}\")\n",
    "        \n",
    "        # Validaciones pre-carga\n",
    "        if structured_data.empty:\n",
    "            logger.warning(\"No hay datos para cargar\")\n",
    "            return False\n",
    "        \n",
    "        print(\"--- Validaciones Pre-Carga ---\")\n",
    "        print(f\"Registros a cargar: {len(structured_data)}\")\n",
    "        print(f\"Columnas: {list(structured_data.columns)}\")\n",
    "        \n",
    "        # Verificar duplicados\n",
    "        duplicates = structured_data.duplicated(subset=['user_id']).sum()\n",
    "        if duplicates > 0:\n",
    "            print(f\"⚠️  Encontrados {duplicates} registros duplicados\")\n",
    "            structured_data = structured_data.drop_duplicates(subset=['user_id'], keep='last')\n",
    "        \n",
    "        # Simular diferentes tipos de carga\n",
    "        if load_type == 'full':\n",
    "            # Carga completa - reemplaza todos los datos\n",
    "            filename = 'data_warehouse_users_full.csv'\n",
    "            structured_data.to_csv(filename, index=False)\n",
    "            print(f\"✅ Carga completa realizada: {filename}\")\n",
    "            \n",
    "        elif load_type == 'incremental':\n",
    "            # Carga incremental - solo nuevos registros\n",
    "            filename = 'data_warehouse_users_incremental.csv'\n",
    "            # En un caso real, verificaríamos qué registros ya existen\n",
    "            structured_data.to_csv(filename, mode='a', header=False, index=False)\n",
    "            print(f\"✅ Carga incremental realizada: {filename}\")\n",
    "            \n",
    "        elif load_type == 'upsert':\n",
    "            # Upsert - insertar nuevos, actualizar existentes\n",
    "            filename = 'data_warehouse_users_upsert.csv'\n",
    "            structured_data.to_csv(filename, index=False)\n",
    "            print(f\"✅ Upsert realizado: {filename}\")\n",
    "        \n",
    "        print(\"\\n--- 4. Datos Cargados ---\")\n",
    "        print(f\"Registros cargados exitosamente: {len(structured_data)}\")\n",
    "        print(\"\\nPrimeras filas del archivo de destino:\")\n",
    "        \n",
    "        # Mostrar contenido del archivo\n",
    "        with open(filename, 'r') as f:\n",
    "            lines = f.readlines()[:6]  # Mostrar solo las primeras 6 líneas\n",
    "            print(''.join(lines))\n",
    "        \n",
    "        # Estadísticas de carga\n",
    "        print(\"\\n--- Estadísticas de Carga ---\")\n",
    "        print(f\"📊 Total de registros: {len(structured_data)}\")\n",
    "        print(f\"📊 Países únicos: {structured_data['country_code'].nunique()}\")\n",
    "        print(f\"📊 Gasto promedio: ${structured_data['total_spent'].mean():.2f}\")\n",
    "        print(f\"📊 Gasto total: ${structured_data['total_spent'].sum():.2f}\")\n",
    "        \n",
    "        logger.info(f\"Carga completada exitosamente: {len(structured_data)} registros\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error en la carga: {str(e)}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6f0dd3",
   "metadata": {},
   "source": [
    "## Ejecución del Pipeline Completo\n",
    "\n",
    "Finalmente, orquestamos todas las funciones en el pipeline principal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7fef5a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-31 16:07:09,243 - INFO - Iniciando extracción de datos...\n",
      "2025-08-31 16:07:09,247 - INFO - Extracción completada: 6 registros\n",
      "2025-08-31 16:07:09,248 - INFO - Iniciando transformación de datos...\n",
      "2025-08-31 16:07:09,251 - INFO - Transformación completada: 3 registros válidos\n",
      "2025-08-31 16:07:09,252 - INFO - Iniciando normalización del esquema...\n",
      "2025-08-31 16:07:09,257 - INFO - Normalización completada: 3 registros estructurados\n",
      "2025-08-31 16:07:09,257 - INFO - Iniciando carga de datos - Tipo: full\n",
      "2025-08-31 16:07:09,259 - INFO - Carga completada exitosamente: 3 registros\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Iniciando Pipeline ETL\n",
      "⏰ Hora de inicio: 2025-08-31 16:07:09.243120\n",
      "==================================================\n",
      "--- 1. Datos Crudos Extraídos ---\n",
      "Registros extraídos: 6\n",
      "Columnas: ['ID_USER', 'user_name', 'registration_date', 'total_spent', 'country_code', 'email']\n",
      "\n",
      "Primeras filas:\n",
      "   ID_USER user_name registration_date total_spent country_code  \\\n",
      "0      101       Ana        2025-01-15       150.5           ES   \n",
      "1      102      Luis        2025-02-20        80.0           MX   \n",
      "2      103     Marta        2025-03-01        -999           ES   \n",
      "3      104      Juan        2025-04-10       200.0           CO   \n",
      "4      105       Eva        2025-05-19       45.25           es   \n",
      "5      106      None        2025-06-30     invalid           ES   \n",
      "\n",
      "             email  \n",
      "0    ana@email.com  \n",
      "1   luis@email.com  \n",
      "2  marta@email.com  \n",
      "3   juan@email.com  \n",
      "4    eva@email.com  \n",
      "5  pedro@email.com  \n",
      "\n",
      "Información del DataFrame:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6 entries, 0 to 5\n",
      "Data columns (total 6 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   ID_USER            6 non-null      int64 \n",
      " 1   user_name          5 non-null      object\n",
      " 2   registration_date  6 non-null      object\n",
      " 3   total_spent        6 non-null      object\n",
      " 4   country_code       6 non-null      object\n",
      " 5   email              6 non-null      object\n",
      "dtypes: int64(1), object(5)\n",
      "memory usage: 420.0+ bytes\n",
      "None\n",
      "\n",
      "--- Análisis de Calidad de Datos ---\n",
      "Registros iniciales: 6\n",
      "Valores nulos por columna:\n",
      "ID_USER              0\n",
      "user_name            1\n",
      "registration_date    0\n",
      "total_spent          0\n",
      "country_code         0\n",
      "email                0\n",
      "dtype: int64\n",
      "\n",
      "⚠️  Encontrados 1 nombres de usuario nulos\n",
      "⚠️  Encontrados 1 valores inválidos en total_spent\n",
      "📊 Registros filtrados (gasto <= 50): 3\n",
      "📊 Registros finales: 3\n",
      "\n",
      "--- 2. Datos Limpios (Reglas Aplicadas) ---\n",
      "   ID_USER user_name registration_date  total_spent country_code  \\\n",
      "0      101       Ana        2025-01-15        150.5           ES   \n",
      "1      102      Luis        2025-02-20         80.0           MX   \n",
      "3      104      Juan        2025-04-10        200.0           CO   \n",
      "\n",
      "            email  \n",
      "0   ana@email.com  \n",
      "1  luis@email.com  \n",
      "3  juan@email.com  \n",
      "\n",
      "--- Análisis del Esquema Original ---\n",
      "Columnas originales: ['ID_USER', 'user_name', 'registration_date', 'total_spent', 'country_code', 'email']\n",
      "Tipos de datos originales:\n",
      "ID_USER                int64\n",
      "user_name             object\n",
      "registration_date     object\n",
      "total_spent          float64\n",
      "country_code          object\n",
      "email                 object\n",
      "dtype: object\n",
      "\n",
      "--- 3. Datos Estructurados (Esquema Normalizado) ---\n",
      "Columnas finales: ['user_id', 'user_name', 'email_address', 'country_code', 'registration_date', 'total_spent', 'processed_at', 'data_source']\n",
      "   user_id user_name   email_address country_code registration_date  \\\n",
      "0      101       Ana   ana@email.com           ES        2025-01-15   \n",
      "1      102      Luis  luis@email.com           MX        2025-02-20   \n",
      "3      104      Juan  juan@email.com           CO        2025-04-10   \n",
      "\n",
      "   total_spent               processed_at    data_source  \n",
      "0        150.5 2025-08-31 16:07:09.253876  user_database  \n",
      "1         80.0 2025-08-31 16:07:09.253876  user_database  \n",
      "3        200.0 2025-08-31 16:07:09.253876  user_database  \n",
      "\n",
      "Información del esquema final:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 3 entries, 0 to 3\n",
      "Data columns (total 8 columns):\n",
      " #   Column             Non-Null Count  Dtype         \n",
      "---  ------             --------------  -----         \n",
      " 0   user_id            3 non-null      int64         \n",
      " 1   user_name          3 non-null      object        \n",
      " 2   email_address      3 non-null      object        \n",
      " 3   country_code       3 non-null      object        \n",
      " 4   registration_date  3 non-null      datetime64[ns]\n",
      " 5   total_spent        3 non-null      float64       \n",
      " 6   processed_at       3 non-null      datetime64[us]\n",
      " 7   data_source        3 non-null      object        \n",
      "dtypes: datetime64[ns](1), datetime64[us](1), float64(1), int64(1), object(4)\n",
      "memory usage: 216.0+ bytes\n",
      "None\n",
      "\n",
      "--- Validaciones Pre-Carga ---\n",
      "Registros a cargar: 3\n",
      "Columnas: ['user_id', 'user_name', 'email_address', 'country_code', 'registration_date', 'total_spent', 'processed_at', 'data_source']\n",
      "✅ Carga completa realizada: data_warehouse_users_full.csv\n",
      "\n",
      "--- 4. Datos Cargados ---\n",
      "Registros cargados exitosamente: 3\n",
      "\n",
      "Primeras filas del archivo de destino:\n",
      "user_id,user_name,email_address,country_code,registration_date,total_spent,processed_at,data_source\n",
      "101,Ana,ana@email.com,ES,2025-01-15,150.5,2025-08-31 16:07:09.253876,user_database\n",
      "102,Luis,luis@email.com,MX,2025-02-20,80.0,2025-08-31 16:07:09.253876,user_database\n",
      "104,Juan,juan@email.com,CO,2025-04-10,200.0,2025-08-31 16:07:09.253876,user_database\n",
      "\n",
      "\n",
      "--- Estadísticas de Carga ---\n",
      "📊 Total de registros: 3\n",
      "📊 Países únicos: 3\n",
      "📊 Gasto promedio: $143.50\n",
      "📊 Gasto total: $430.50\n",
      "\n",
      "==================================================\n",
      "📊 RESUMEN DEL PIPELINE ETL\n",
      "==================================================\n",
      "✅ Estado: EXITOSO\n",
      "⏱️  Duración total: 0.02 segundos\n",
      "📥 Registros extraídos: 6\n",
      "🔄 Registros procesados: 3\n",
      "📤 Registros cargados: 3\n",
      "📉 Tasa de filtrado: 50.0%\n"
     ]
    }
   ],
   "source": [
    "def etl_pipeline_with_monitoring():\n",
    "    \"\"\"\n",
    "    Pipeline ETL completo con monitoreo y manejo de errores.\n",
    "    \"\"\"\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    try:\n",
    "        print(\"🚀 Iniciando Pipeline ETL\")\n",
    "        print(f\"⏰ Hora de inicio: {start_time}\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Extract\n",
    "        raw_data = extract_from_database()\n",
    "        \n",
    "        # Transform\n",
    "        cleaned_data = apply_business_rules(raw_data)\n",
    "        \n",
    "        # Normalize\n",
    "        structured_data = normalize_schema(cleaned_data)\n",
    "        \n",
    "        # Load\n",
    "        success = load_to_warehouse(structured_data, load_type='full')\n",
    "        \n",
    "        # Métricas finales\n",
    "        end_time = datetime.now()\n",
    "        duration = end_time - start_time\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\"📊 RESUMEN DEL PIPELINE ETL\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"✅ Estado: {'EXITOSO' if success else 'FALLIDO'}\")\n",
    "        print(f\"⏱️  Duración total: {duration.total_seconds():.2f} segundos\")\n",
    "        print(f\"📥 Registros extraídos: {len(raw_data)}\")\n",
    "        print(f\"🔄 Registros procesados: {len(cleaned_data)}\")\n",
    "        print(f\"📤 Registros cargados: {len(structured_data)}\")\n",
    "        print(f\"📉 Tasa de filtrado: {((len(raw_data) - len(structured_data)) / len(raw_data) * 100):.1f}%\")\n",
    "        \n",
    "        return structured_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Pipeline falló: {str(e)}\")\n",
    "        print(f\"❌ Error en el pipeline: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Ejecutar el pipeline completo\n",
    "result = etl_pipeline_with_monitoring()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c50e4c7",
   "metadata": {},
   "source": [
    "## 🔧 Ejecución Paso a Paso\n",
    "\n",
    "Si quieres ver cada paso por separado, ejecuta las siguientes celdas una por una:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "69763808",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-31 16:07:40,174 - INFO - Iniciando extracción de datos...\n",
      "2025-08-31 16:07:40,180 - INFO - Extracción completada: 6 registros\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PASO 1: EXTRACCIÓN ===\n",
      "--- 1. Datos Crudos Extraídos ---\n",
      "Registros extraídos: 6\n",
      "Columnas: ['ID_USER', 'user_name', 'registration_date', 'total_spent', 'country_code', 'email']\n",
      "\n",
      "Primeras filas:\n",
      "   ID_USER user_name registration_date total_spent country_code  \\\n",
      "0      101       Ana        2025-01-15       150.5           ES   \n",
      "1      102      Luis        2025-02-20        80.0           MX   \n",
      "2      103     Marta        2025-03-01        -999           ES   \n",
      "3      104      Juan        2025-04-10       200.0           CO   \n",
      "4      105       Eva        2025-05-19       45.25           es   \n",
      "5      106      None        2025-06-30     invalid           ES   \n",
      "\n",
      "             email  \n",
      "0    ana@email.com  \n",
      "1   luis@email.com  \n",
      "2  marta@email.com  \n",
      "3   juan@email.com  \n",
      "4    eva@email.com  \n",
      "5  pedro@email.com  \n",
      "\n",
      "Información del DataFrame:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6 entries, 0 to 5\n",
      "Data columns (total 6 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   ID_USER            6 non-null      int64 \n",
      " 1   user_name          5 non-null      object\n",
      " 2   registration_date  6 non-null      object\n",
      " 3   total_spent        6 non-null      object\n",
      " 4   country_code       6 non-null      object\n",
      " 5   email              6 non-null      object\n",
      "dtypes: int64(1), object(5)\n",
      "memory usage: 420.0+ bytes\n",
      "None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Paso 1: Extracción\n",
    "print(\"=== PASO 1: EXTRACCIÓN ===\")\n",
    "raw_data_step = extract_from_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a1633ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-31 16:07:48,604 - INFO - Iniciando transformación de datos...\n",
      "2025-08-31 16:07:48,607 - INFO - Transformación completada: 3 registros válidos\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PASO 2: TRANSFORMACIÓN ===\n",
      "--- Análisis de Calidad de Datos ---\n",
      "Registros iniciales: 6\n",
      "Valores nulos por columna:\n",
      "ID_USER              0\n",
      "user_name            1\n",
      "registration_date    0\n",
      "total_spent          0\n",
      "country_code         0\n",
      "email                0\n",
      "dtype: int64\n",
      "\n",
      "⚠️  Encontrados 1 nombres de usuario nulos\n",
      "⚠️  Encontrados 1 valores inválidos en total_spent\n",
      "📊 Registros filtrados (gasto <= 50): 3\n",
      "📊 Registros finales: 3\n",
      "\n",
      "--- 2. Datos Limpios (Reglas Aplicadas) ---\n",
      "   ID_USER user_name registration_date  total_spent country_code  \\\n",
      "0      101       Ana        2025-01-15        150.5           ES   \n",
      "1      102      Luis        2025-02-20         80.0           MX   \n",
      "3      104      Juan        2025-04-10        200.0           CO   \n",
      "\n",
      "            email  \n",
      "0   ana@email.com  \n",
      "1  luis@email.com  \n",
      "3  juan@email.com  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Paso 2: Transformación\n",
    "print(\"=== PASO 2: TRANSFORMACIÓN ===\")\n",
    "cleaned_data_step = apply_business_rules(raw_data_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "457582c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-31 16:07:56,773 - INFO - Iniciando normalización del esquema...\n",
      "2025-08-31 16:07:56,779 - INFO - Normalización completada: 3 registros estructurados\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PASO 3: NORMALIZACIÓN ===\n",
      "--- Análisis del Esquema Original ---\n",
      "Columnas originales: ['ID_USER', 'user_name', 'registration_date', 'total_spent', 'country_code', 'email']\n",
      "Tipos de datos originales:\n",
      "ID_USER                int64\n",
      "user_name             object\n",
      "registration_date     object\n",
      "total_spent          float64\n",
      "country_code          object\n",
      "email                 object\n",
      "dtype: object\n",
      "\n",
      "--- 3. Datos Estructurados (Esquema Normalizado) ---\n",
      "Columnas finales: ['user_id', 'user_name', 'email_address', 'country_code', 'registration_date', 'total_spent', 'processed_at', 'data_source']\n",
      "   user_id user_name   email_address country_code registration_date  \\\n",
      "0      101       Ana   ana@email.com           ES        2025-01-15   \n",
      "1      102      Luis  luis@email.com           MX        2025-02-20   \n",
      "3      104      Juan  juan@email.com           CO        2025-04-10   \n",
      "\n",
      "   total_spent               processed_at    data_source  \n",
      "0        150.5 2025-08-31 16:07:56.775963  user_database  \n",
      "1         80.0 2025-08-31 16:07:56.775963  user_database  \n",
      "3        200.0 2025-08-31 16:07:56.775963  user_database  \n",
      "\n",
      "Información del esquema final:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 3 entries, 0 to 3\n",
      "Data columns (total 8 columns):\n",
      " #   Column             Non-Null Count  Dtype         \n",
      "---  ------             --------------  -----         \n",
      " 0   user_id            3 non-null      int64         \n",
      " 1   user_name          3 non-null      object        \n",
      " 2   email_address      3 non-null      object        \n",
      " 3   country_code       3 non-null      object        \n",
      " 4   registration_date  3 non-null      datetime64[ns]\n",
      " 5   total_spent        3 non-null      float64       \n",
      " 6   processed_at       3 non-null      datetime64[us]\n",
      " 7   data_source        3 non-null      object        \n",
      "dtypes: datetime64[ns](1), datetime64[us](1), float64(1), int64(1), object(4)\n",
      "memory usage: 216.0+ bytes\n",
      "None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Paso 3: Normalización\n",
    "print(\"=== PASO 3: NORMALIZACIÓN ===\")\n",
    "structured_data_step = normalize_schema(cleaned_data_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2cbcc55d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-31 16:08:02,755 - INFO - Iniciando carga de datos - Tipo: full\n",
      "2025-08-31 16:08:02,759 - INFO - Carga completada exitosamente: 3 registros\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PASO 4: CARGA ===\n",
      "--- Validaciones Pre-Carga ---\n",
      "Registros a cargar: 3\n",
      "Columnas: ['user_id', 'user_name', 'email_address', 'country_code', 'registration_date', 'total_spent', 'processed_at', 'data_source']\n",
      "✅ Carga completa realizada: data_warehouse_users_full.csv\n",
      "\n",
      "--- 4. Datos Cargados ---\n",
      "Registros cargados exitosamente: 3\n",
      "\n",
      "Primeras filas del archivo de destino:\n",
      "user_id,user_name,email_address,country_code,registration_date,total_spent,processed_at,data_source\n",
      "101,Ana,ana@email.com,ES,2025-01-15,150.5,2025-08-31 16:07:56.775963,user_database\n",
      "102,Luis,luis@email.com,MX,2025-02-20,80.0,2025-08-31 16:07:56.775963,user_database\n",
      "104,Juan,juan@email.com,CO,2025-04-10,200.0,2025-08-31 16:07:56.775963,user_database\n",
      "\n",
      "\n",
      "--- Estadísticas de Carga ---\n",
      "📊 Total de registros: 3\n",
      "📊 Países únicos: 3\n",
      "📊 Gasto promedio: $143.50\n",
      "📊 Gasto total: $430.50\n",
      "\n",
      "✅ Pipeline completado exitosamente: True\n"
     ]
    }
   ],
   "source": [
    "# Paso 4: Carga\n",
    "print(\"=== PASO 4: CARGA ===\")\n",
    "success = load_to_warehouse(structured_data_step, load_type='full')\n",
    "print(f\"\\n✅ Pipeline completado exitosamente: {success}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
